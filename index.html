<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=us-ascii">
    <meta name="viewport" content="&acirc;&#8364;&#339;width=800&acirc;&#8364;&#157;">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    .auto-style1 {
		color: #1772D0;
	}
  .auto-style2 {
	text-align: right;
}
  </style>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <title>Ziyan Wu</title>
    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic"
      rel="stylesheet"
      type="text/css">
  </head>
  <body>
    <table width="800" cellspacing="0" cellpadding="0" border="0" align="center">
      <tbody>
        <tr>
          <td>
            <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
              <tbody>
                <tr>
                  <td width="67%" valign="middle">
                    <p align="center"> <name>Ziyan Wu</name> </p>
                    <p>I am a Principal Expert Scientist at 
					<a href="https://www.linkedin.com/company/uii-america-inc">UII America</a> 
					in Cambridge, MA, where I work on computer vision and 
					machine learning problems in medical environments. Before 
					joining UII I have worked at Siemens and Honeywell 
					repsectively. </p>
                    <p> I received my PhD in Computer and Systems Engineering
                      from <a href="http://www.rpi.edu">Rensselaer Polytechnic
                        Institute</a> in May 2014 under the supervision of <a href="https://www.ecse.rpi.edu/%7Erjradke/index.htm">Prof.
                        Richard J. Radke</a>. I was affiliated with 
					<a href="http://www.northeastern.edu/alert/">DHS Center 
					of Excellence on Awareness and Localization of 
					Explosives-Related Threats</a> (ALERT)
                      as a PhD researcher from 2009 to 2014.</p>
                    <p>I grew up in Guangzhou, China. Before coming to US, I have spent 7 years in Beijing where I received my B.S and M.S from <a href="http://www.buaa.edu.cn">Beihang
                        University</a> (BUAA), in 2006 and 2009
                      respectively.&nbsp; <span style="caret-color: rgb(245, 243, 243); color: rgb(245, 243, 243); font-family: Arial, Helvetica, sans-serif; font-size: 13.333333015441895px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; background-color: rgb(13, 13, 13); text-decoration: none; display: inline !important; float: none;"></span></p>
                    <p align="center"> &nbsp;<a href="https://scholar.google.com/citations?user=CkPUb-4AAAAJ">Google
                        Scholar</a> &nbsp;/&nbsp; <a href="http://www.linkedin.com/in/wuziyan/">
                        LinkedIn</a> &nbsp;/ &nbsp;<a href="mailto:ziyan@alum.rpi.edu">Email</a></p>
                  </td>
                  <td width="33%"> 
				  <img src="ziyan_wu.jpg" height="240" width="231"> </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
              <tbody>
                <tr>
                  <td width="100%" valign="middle"> <heading>News</heading>
                    <ul>
						<li>The Vision and Robotics team at UII America in 
						Cambridge, MA is hiring! We 
						have openings for
						Sr. Research Scientist and
						<a href="https://www.indeed.com/company/UII-America,-Inc./jobs/Computer-Vision-Research-Intern-a879144fd2e82724?fccid=45676318e96777e1&vjs=3">
						Research Intern</a>.</li>
						<li>We are organizing the
						<a href="https://wvbsd.github.io/2019/index.html">2nd 
						Workshop on Vision with Biased and Scarce Data</a>, in 
						conjunction with CVPR 2019. </li>
				  </ul>
                  </td>
                </tr>
                <tr>
                  <td width="100%" valign="middle"> <heading>Research</heading>
                    <p>My research interests are computer vision and machine
                      learning, with special focus on object detection and
                      tracking, anomaly detection, augmented reality, scene
                      understanding, human re-identification and camera calibration.</p>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
              <tbody>
                  <tr>
                  <td width="25%">
                    <img src="iss.JPG" height="157" width="157">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="https://arxiv.org/abs/1811.12297">
					<papertitle>Incremental Scene Synthesis</papertitle></a><br>
                      <a href="http://planche.me/">Benjamin Planche</a>, 
					<a href="https://xrong.org/">Xuejian 
					Rong</a>,  <strong>Ziyan Wu</strong>, 
				    <span>
                      <a href="https://karanams.github.io/">Srikrishna Karanam</a>,<a href="http://www.fim.uni-passau.de/en/distributed-information-systems/">Harald Kosch</a>,&nbsp;<a href="http://scholar.google.com/citations?user=aAWeB4wAAAAJ&amp;hl=en">YingLi Tian</a>,<a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> Jan Ernst</a>, </span> 
				  <a href="https://www.researchgate.net/profile/Andreas_Hutter">Andreas Hutter</a><br>
                      <em>Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2019 <br>
					</p>
                    <p>We present a method to incrementally generate complete 2D or 3D scenes with global consistentcy at each step according to a
learned scene prior. Real observations of a scene can be incorporated while observing global consistency and unobserved regions can be hallucinated locally in
consistence with previous observations, hallucinations as well as global priors. Hallucinations are statistical in nature, i.e., different scenes can be generated from
the same observations. </p>
                  </td>
                  </tr>
                  <tr>
                  <td width="25%" style="height: 200px">
						  <img src="ICASC.png" height="160" width="160"></td>
                  <td width="75%" valign="top"> <p> <a href="https://arxiv.org/abs/1811.07484">
                      <papertitle>Sharpen Focus: Learning with Attention Separability and Consistency</papertitle>
                    </a><br>
                    <span><a href="http://wanglezi.github.io/">Lezi Wang</a>, 
				  <strong>Ziyan Wu</strong>,
                      <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
                      <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a>,  
					<a href="http://rajatvikramsingh.github.io/">Rajat Vikram Singh</a>,
				  <a href="http://scholar.google.com/citations?user=2Fe93n8AAAAJ&amp;hl=en">Bo Liu</a>,
				  <a href="https://www.cs.rutgers.edu/~dnm/">Dimitris N. Metaxas</a></span>&nbsp;<br>
                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019 <br></p>
                    <p>We improve the generalizability of CNNs by means of a new framework that makes class-discriminative attention a principled part of the learning process. We propose new learning objectives for attention separability and cross-layer consistency, which result in improved attention discriminability and reduced visual
confusion. </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%" style="height: 200px">
						  <img src="RGB2CAD.png" height="160" width="160"></td>
                  <td width="75%" valign="top"> <p><a href="https://arxiv.org/abs/1811.07249">
                      <papertitle>Learning Local RGB-to-CAD Correspondences for Object Pose Estimation</papertitle>
                    </a><br>
                     <a href="https://cs.gmu.edu/~ggeorgak/">Georgios Georgakis</a>, 
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>,
					<strong>Ziyan Wu</strong>,
				  <a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a><br></p>
                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019 <br>
                    <p>We solve the key problem of existing 3D object pose estimation methods requiring expensive 3D pose annotations by proposing a new method that matches RGB images to CAD models for object pose estimation. 
                    Our method requires neither real-world textures for CAD models nor explicit 3D pose annotations for RGB images.</p>
                  </td>
                  </tr>
                <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
                  <td width="25%" style="height: 200px">
						  <img src="gain_pami19.JPG" height="158" width="160"></td>
                  <td width="75%" valign="top"> <p><a href="https://ieeexplore.ieee.org/document/8733010">
                      <papertitle>Guided Attention Inference Network</papertitle>
                    </a><br>
                    <span><a href="https://kunpengli1994.github.io/">Kunpeng Li</a>, 
				  <strong>Ziyan Wu</strong>,
                      <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a>,<a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> Jan Ernst</a>, 
				  <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a></span>
                    <br>
                    <em>IEEE Transactions on Pattern Analysis and Machine
                      Intelligence (<strong>TPAMI</strong>)</em>, to appear, 2019 <br></p>
                    <p>This is an extension of our 
					<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Tell_Me_Where_CVPR_2018_paper.pdf">CVPR 18 work</a> with added 
					support of bounding box labels seamlessly integrated with 
					image level and pixel level labels for weakly supervised 
					semantic segmentation. </p>
                  </td>
                </tr>
                                <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
                  <td width="25%" style="height: 200px">
						  <img src="synDA_iros19.JPG" height="113" width="160"></td>
                  <td width="75%" valign="top"> <p><a href="https://arxiv.org/abs/1804.09113">
                      <papertitle>Seeing Beyond Appearance - Mapping Real Images into Geometrical Domains for Unsupervised CAD-based Recognition</papertitle>
                    </a><br>
                    <span><a href="http://planche.me/">Benjamin Planche</a>*, <a href="http://campar.in.tum.de/Main/SergeyZakharov">Sergey Zakharov</a>*,  <strong>Ziyan Wu</strong>, 
				  <a href="https://www.researchgate.net/profile/Andreas_Hutter">Andreas Hutter</a>, 
				  <a href="http://www.fim.uni-passau.de/en/distributed-information-systems/">Harald Kosch</a>,
				  <a href="http://campar.in.tum.de/Main/SlobodanIlic">Slobodan Ilic</a></span>
                    <br>
                    <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2019<em> 
				  </em></p>
                    <p>We introduce a pipeline to map unseen target samples into the 
					synthetic domain used to train task-specific methods. 
					Denoising the data and retaining only the features these 
					recognition algorithms are familiar with.</p>
                  </td>
                </tr>

                <tr onmouseout="friendly_stop()" onmouseover="friendly_start()">
                  <td width="25%">
                    <img src="lwm_cvpr19.JPG" height="157" width="157">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Dhar_Learning_Without_Memorizing_CVPR_2019_paper.pdf">
                        <papertitle>Learning without Memorizing</papertitle></a><br>
                      <a href="https://sites.google.com/site/prithvirajdhar274/">Prithviraj Dhar</a>*, 
					<a href="http://rajatvikramsingh.github.io/">Rajat Vikram Singh</a>*,<a href="http://homes.cs.washington.edu/%7Earmin/">  </a>
                    <span>
                      <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a></span>, <strong>Ziyan Wu</strong>, 
					<a href="http://users.umiacs.umd.edu/~rama/">Rama Chellappa</a><br>
                      <em>IEEE/CVF Conference on Computer Vision and Pattern 
					Recognition (<strong>CVPR</strong>)</em>, 2019 <br>
					</p>
                    <p>Knowledge distillation should not only focus on &quot;what&quot;, 
					but also &quot;why&quot;. We peoposed an online learning method to 
					preserve the exisiting knowledge&nbsp; without storing any 
					data, while making the classifier progressively learn to 
					encode the new classes.</p>
                  </td>
                </tr>
                <tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()">
                  <td width="25%">
                    &nbsp;
                    <img src="ASN_cvpr19.JPG" height="157" width="138">
                     </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Re-Identification_With_Consistent_Attentive_Siamese_Networks_CVPR_2019_paper.pdf">
                        <papertitle>Re-identification with Consistent Attentive Siamese Networks</papertitle></a><br>
                      <a href="http://homepages.rpi.edu/~zhengm3/">Meng Zheng</a>, 
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>,<a href="http://homes.cs.washington.edu/%7Earmin/">  </a>
					<strong>Ziyan Wu</strong>, 
					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a><br>
                      <em>IEEE/CVF Conference on Computer Vision and Pattern 
					Recognition (<strong>CVPR</strong>)</em>, 2019 <br>
                    </p>
                    <p>We proposed the first learning architecture that integrates attention consistency modeling and Siamese representation learning in a joint learning framework, called the Consistent Attentive Siamese Network (CASN), for person re-id.</p>
                  </td>
                </tr>
                <tr onmouseout="deepburst_stop()" onmouseover="deepburst_start()">
                  <td width="25%">
                    <img src="counterfactual_icml19.JPG" height="157" width="158">
                     </td>
                  <td width="75%" valign="top"> <a href="http://proceedings.mlr.press/v97/goyal19a/goyal19a.pdf">
                      <papertitle>Counterfactual Visual Explanations</papertitle>
                    </a><br>
                    <a href="https://www.cc.gatech.edu/~ygoyal3/">Yash Goyal</a>, <strong>Ziyan Wu</strong>, 
				  <span>
				  <a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en">Jan Ernst</a></span>, 
				  <a href="https://computing.ece.vt.edu/~dbatra/">Dhruv Batra</a>, 
				  <a href="https://computing.ece.vt.edu/~parikh/">Devi Parikh</a>, 
				  <a href="https://www.cc.gatech.edu/~slee3191/">Stefan Lee</a>&nbsp; <br>
                    <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2019
                    <br>
                    <p>
					<a href="https://icml.cc/media/Slides/icml/2019/seasideball(12-16-00)-12-17-10-5175-counterfactual_.pdf">
					slides</a> /
					<a href="http://proceedings.mlr.press/v97/goyal19a/goyal19a-supp.pdf">
					supplementary</a></p>
                    <p>A technique to produce counterfactual visual explanations. Given a 'query' image I for which a vision system predicts class c, a counterfactual visual explanation identifies how I could change such that the system would output a different specified class c&#8242;. </p>
                  </td>
                </tr>
                
                 <tr>
                  <td width="25%"> 
				  <img src="benchmark_pami19.JPG" height="132" width="161"> </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-pami18.pdf">
                        <papertitle>A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets</papertitle></a><br>
                    <span>
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>*,
					<a href="https://neu-gou.github.io/">Mengran Gou</a>*,
					<strong>Ziyan Wu</strong>, Angels Rates-Borras,
					<a href="http://www.ece.neu.edu/people/camps-octavia">
					Octavia Camps</a>,
					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a><br>
                    <em>IEEE Transactions on Pattern Analysis and Machine
                      Intelligence (<strong>TPAMI</strong>)</em>, Vol. 41, No. 3, pp. 523-536, March 2019   
                    </span>
                    </p>
					<p>

					<a href="https://arxiv.org/abs/1605.09653">supplementary</a> 
					/
					<a href="http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/">
					dataset</a> / 
					<a href="https://github.com/RSL-NEU/person-reid-benchmark">
					code</a>
                    </p>
                    <p>We present an extensive review and
performance evaluation of single and multi-shot re-id algorithms. The experimental protocol incorporates 11 feature extraction 
					and 22 metric learning and ranking techniques and evaluates
using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 16 other publicly available datasets. </p>
                  </td>
                </tr>

                <tr>
                  <td width="25%"> 
				  <img src="zdda_eccv18.JPG" height="132" width="161"> </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Kuan-Chuan_Peng_Zero-Shot_Deep_Domain_ECCV_2018_paper.pdf">
                        <papertitle>Zero Shot Deep Domain Adaptation</papertitle></a><br>
                    <span>
                      <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a></span>, <strong>Ziyan Wu</strong>,
					<span>
					<a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en">Jan Ernst</a></span><br>
                      <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2018  
                    <br>
                    </p>
                    <p>We propose zero-shot deep domain adaptation (ZDDA) for 
					domain adaptation and sensor fusion. ZDDA learns from the 
					task-irrelevant dual-domain pairs when the task-relevant 
					target-domain training data is unavailable. </p>
                  </td>
                </tr>
                <tr onmouseout="ffcc_stop()" onmouseover="ffcc_start()">
                  <td width="25%">
                    <img src="gain_cvpr18.jpg" height="155" width="157">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Tell_Me_Where_CVPR_2018_paper.pdf">
                        <papertitle>Tell Me Where To Look: Guided Attention Inference Network</papertitle></a><br>
                    <span><a href="https://kunpengli1994.github.io/">Kunpeng Li</a>, 
				  <strong>Ziyan Wu</strong>,
                      <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a>,<a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> Jan Ernst</a>, 
				  <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a></span><br>
                      <em>IEEE/CVF Conference on Computer Vision and Pattern 
					Recognition (<strong>CVPR</strong>)</em>, 
					2018 <font color="red"><strong>(spotlight)</strong></font> </p>
					<p>
					<a href="https://github.com/alokwhitewolf/Guided-Attention-Inference-Network">
					<span class="auto-style1">code</span></a> by 
					<a href="https://github.com/alokwhitewolf">alokwhitewolf</a> 
					/ <a href="https://www.youtube.com/watch?v=op9IBox_TTc">
					talk</a></p>
                    <p>In one common framework we address three shortcomings of 
					previous approaches in modeling such attention maps: We (1) 
					first time make attention maps an explicit and natural 
					component of the end-to-end training, (2) provide 
					self-guidance directly on these maps by exploring 
					supervision form the network itself to improve them, and (3) 
					seamlessly bridge the gap between using weak and extra 
					supervision if available.</p>
                  </td>
                </tr>
                
                <tr onmouseout="hdrp_stop()" onmouseover="hdrp_start()">
                  <td width="25%">
                    <img src="conceptgan_cvpr18.JPG" height="157" width="159">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1959.pdf">
                        <papertitle>Learning Compositional Visual Concepts with Mutual Consistency</papertitle></a><br>
                     <a href="https://www.ece.cornell.edu/research/grad-students/yunye-gong">Yunye Gong</a>, 
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>,
					<strong>Ziyan Wu</strong>, 
                    <span>
                      <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a></span>,<span><a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> Jan Ernst</a></span>, 
					<a href="https://www.bme.cornell.edu/faculty-directory/peter-doerschuk">Peter C. Doerschuk</a><br>
                      <em>IEEE/CVF Conference on Computer Vision and Pattern 
					Recognition (<strong>CVPR</strong>)</em>, 2018 <font color="red"><strong>(spotlight)</strong></font> <br>
                    </p>
                    <p><a href="https://www.youtube.com/watch?v=mDVD13M2X94">
					video</a></p>
                    <p>We proposed ConceptGAN, a novel concept learning 
					framework where we seek to capture underlying semantic 
					shifts between data domains instead of mappings restricted 
					to training distributions. The key idea is that via joint 
					concept learning, transfer and composition, information over 
					a joint latent space is recovered given incomplete training 
					data.</p>
                  </td>
                </tr>
                                <tr onmouseout="diverdi_stop()" onmouseover="diverdi_start()">
                  <td width="25%" style="height: 248px">
                    <img src="e2ekeypoint_cvpr18.JPG" height="143" width="156">
                    </td>
                  <td width="75%" valign="top">
                    <p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.pdf">
                        <papertitle>End-to-End Learning of Keypoint Detector and Descriptor for Pose Invariant 3D Matching</papertitle></a><br>
                     <a href="https://cs.gmu.edu/~ggeorgak/">Georgios Georgakis</a>, 
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>,
					<strong>Ziyan Wu</strong>,<span><a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> Jan Ernst</a></span>, 
					<a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a><br>
                      <em>IEEE/CVF Conference on Computer Vision and Pattern 
					Recognition (<strong>CVPR</strong>)</em>, 2018</p>
					<p>Related Product: Siemens
					<a href="https://new.siemens.com/global/en/products/mobility/rail-solutions/services/spare-part-services/easy-spares-idea.html">EasySpareIDea&reg;</a><br>
					</p>
                    <p>We proposed an end-to-end learning
framework for keypoint detection and its representation (descriptor) for 3D depth maps or 3D scans, where the two can
be jointly optimized towards task-specific objectives without
a need for separate annotations. </p>
                  </td>
                </tr>
                  <tr>
                  <td width="25%">
                    <img src="affinehull.png" height="160" width="160">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-tcsvt17.pdf">
                        <papertitle>Learning Affine Hull Representations for Multi-Shot Person Re-Identification</papertitle></a><br>
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
                      <strong>Ziyan Wu</strong>,<span><a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> </a></span> 
					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a><br>
                      <em>IEEE Transactions on Circuits and Systems for Video Technology (</em><strong><em>TCSVT</em></strong><em>), 
					Vol.28, No.10, pp.2500-2512, Oct 2018</em><br>
                 
                    <p>We describe the image sequence data using affine hulls, 
					and we show that directly
computing the distance between the closest points on these affine
hulls as in existing recognition algorithms is not sufficiently
discriminative in the context of person re-identification. To this
end, we incorporate affine hull data modeling into the traditional
distance metric learning framework, learning discriminative
feature representations directly using affine hulls. </p>
                  </td>
                  </tr>
                 <tr onmouseout="diverdi_stop()" onmouseover="diverdi_start()">
                  <td width="25%">
                    <img src="keepitunreal_3dv18.JPG" height="156" width="156">
                    </td>
                  <td width="75%" valign="top">
                    <p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.pdf">
                        <papertitle>Keep it Unreal: Bridging the Realism Gap for 2.5D Recognition with Geometry Priors Only</papertitle></a><br>
                    <span><a href="http://campar.in.tum.de/Main/SergeyZakharov">Sergey Zakharov</a>*,  <a href="http://planche.me/">Benjamin Planche</a>*,  <strong>Ziyan Wu</strong>, 
				  <a href="https://www.researchgate.net/profile/Andreas_Hutter">Andreas Hutter</a>, 
				  <a href="http://www.fim.uni-passau.de/en/distributed-information-systems/">Harald Kosch</a>,
				  <a href="http://campar.in.tum.de/Main/SlobodanIlic">Slobodan Ilic</a></span>
                    <br>
                      <em>International Conference on 3D Vision (<strong>3DV</strong>)</em>, 2018 <font color="red"><strong>(oral)</strong></font> <br>
					</p>
                    <p>We propose a novel approach leveraging only CAD models to bridge the realism gap. Purely
trained on synthetic data, playing against an extensive augmentation pipeline in an unsupervised manner, a generative adversarial network learns to effectively segment depth
images and recover the clean synthetic-looking depth information even from partial occlusions. </p>
                  </td>
                </tr>

               
                <tr onmouseout="dt_stop()" onmouseover="dt_start()">
                  <td width="25%">
                    <img src="videosum_iccv17.png" height="154" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Panda_Weakly_Supervised_Summarization_ICCV_2017_paper.pdf">
                        <papertitle>Weakly Supervised Summarization of Web Videos</papertitle></a><br>
                      <a href="https://rpand002.github.io/">Rameswar Panda</a>, <a href="http://cse.iitkgp.ac.in/~adas/">Abir Das</a>, 
                      <strong>Ziyan Wu</strong>,<span><a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> Jan Ernst</a></span>, 
					<a href="https://vcg.engr.ucr.edu/amit">Amit K. Roy-Chowdhury</a> <br>
                      <em>IEEE International Conference on Computer Vision (</em><strong><em>ICCV</em></strong><em>), 
					2017</em><br>
                 
                    <p>
					<a href="https://pdfs.semanticscholar.org/9b25/9b261bf4441230f52bcb74164661e04416ae.pdf">
					supplementary</a></p>
                    <p>We proposed a weakly supervised approach to summarize 
					videos with only video-level annotation, introducing an 
					effective method for computing spatio-temporal importance 
					scores without resorting to additional training steps. </p>
                  </td>
                </tr>
                
                               <tr onmouseout="diverdi_stop()" onmouseover="diverdi_start()">
                  <td width="25%">
                    <img src="depthsynth.png" height="160" width="160">
                    </td>
                  <td width="75%" valign="top">
                    <p><a href="https://arxiv.org/abs/1702.08558">
                        <papertitle>DepthSynth: Real-Time Realistic Synthetic Data Generation from CAD Models for 2.5D Recognition</papertitle></a><br>
                    <span><a href="http://planche.me/">Benjamin Planche</a>*,  <strong>Ziyan Wu</strong>, 
				    <a href="https://scholar.google.com/citations?user=FSSXeyAAAAAJ&amp;hl=en">
					Kai Ma</a>,
					<a href="https://sites.google.com/site/shanhuisun">Shanhui 
					Sun</a>, <a href="http://www.kluckner.com/">Stefan Kluckner</a>,
					<a href="http://www.ifp.illinois.edu/~tchen5/AboutMe.html">
					Terrence Chen</a>,
					<a href="https://www.researchgate.net/profile/Andreas_Hutter">Andreas Hutter</a>, 
				  <a href="http://www.fim.uni-passau.de/en/distributed-information-systems/">Harald Kosch</a>,
				  <a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> Jan Ernst</a></span><br>
                      <em>International Conference on 3D Vision (<strong>3DV</strong>)</em>, 
					2017 <font color="red"><strong>(oral)</strong></font>
					</p>
					<p>Related Product: Siemens
					<a href="https://new.siemens.com/global/en/products/mobility/rail-solutions/services/spare-part-services/easy-spares-idea.html">EasySpareIDea&reg;</a><br>
					</p>
                    <p>We propose an end-to-end framework which simulates the whole mechanism of 3D sensors (structured light and TOF), generating realistic depth data from 3D models by comprehensively modeling vital factors e.g. sensor noise, material reflectance, surface geometry. </p>
                  </td>
                </tr>


              	<tr>
                  <td width="25%">
                    <img src="vessel.png" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ncbi.nlm.nih.gov/pubmed/28413808">
                        <papertitle>Vessel Tree Tracking in Angiographic Sequences</papertitle></a><br>
                      <a href="http://www.dromston.com/">Dong Zhang</a>,
                    <span>
					<a href="https://sites.google.com/site/shanhuisun">Shanhui 
					Sun</a></span>, 
                      <strong>Ziyan Wu</strong>,
					<a href="https://sites.google.com/site/borjengchen/">
					Bor-Jeng Chen</a>, 
					<span>
					<a href="http://www.ifp.illinois.edu/~tchen5/AboutMe.html">
					Terrence Chen</a></span><br>
                      <em>Journal of Medical Imaging (</em><strong><em>JMI</em></strong><em>), 
					Vol.4, No.2, 025001, 2017</em><br>
                 
                    <p>We present a method to track vessels in angiography. Our 
					method maximizes the appearance similarity while preserving 
					the vessel structure. The vessel tree
tracking problem turns into finding the most similar tree from the DAG in the next frame, and it is solved
using an efficient dynamic programming algorithm.  </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%">
                    <img src="lab2realworld.JPG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/radke-csvt16.pdf">
                        <papertitle>From the Lab to the Real World: Re-Identification in an Airport Camera Network</papertitle></a><br>
                    <span>
					<a href="http://www.ece.neu.edu/people/camps-octavia">
					Octavia Camps</a></span>,
                    <span>
					<a href="https://neu-gou.github.io/">Mengran Gou</a></span>, 
					<a href="https://linkedin.com/in/tom-hebble-8b625b55">Tom 
					Hebble</a>,
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
					<a href="https://scholar.google.com/citations?user=dkK56rkAAAAJ&amp;hl=en">Oliver Lehmann</a>,
					<a href="https://www.linkedin.com/in/yangli625">Yang 
					Li</a>,
                    <span>
					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></span>, 
					  <strong>Ziyan Wu</strong>, 
					<a href="https://scholar.google.com/citations?user=zs3rtqcAAAAJ&amp;hl=en">Fei Xiong
					</a> <br>
                      <em>IEEE Transactions on Circuits and Systems for Video Technology (</em><strong><em>TCSVT</em></strong><em>), 
					Vol. 27, No. 3, pp. 540-553, Mar 2017</em><br>
                 
                    <p>We detail the challenges of the real-world airport environment, the computer vision algorithms underlying our human detection and re-identification algorithms,
                     our robust software architecture, and the ground-truthing system required to provide the training and validation data for the algorithms. </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%">
                    <img src="guidewire.jpg" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://ieeexplore.ieee.org/document/7493221">
                        <papertitle>Guidewire Tracking Using a Novel Sequential Segment Optimization Method in Interventional X-Ray Videos</papertitle></a><br>
					<a href="https://sites.google.com/site/borjengchen/">
					Bor-Jeng Chen</a>, 
					  <strong>Ziyan Wu</strong>,
					<span>
					<a href="https://sites.google.com/site/shanhuisun">Shanhui 
					Sun</a></span>, 
                      <a href="http://www.dromston.com/">Dong Zhang</a>,
                    <span>
					<a href="http://www.ifp.illinois.edu/~tchen5/AboutMe.html">
					Terrence Chen</a></span><br>
                      <em>IEEE International Symposium on Biomedical Imaging (</em><strong><em>ISBI</em></strong><em>), 
					2016</em><br>
                 
                    <p>We model the wire-like structure as a sequence of small segments and formulate guidewire tracking as a graph-based optimization problem which aims to find the optimal link set. 
                    To overcome distracters, we extract them from the dominant motion pattern and propose a confidence re-weighting process in the appearance measurement. </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%" style="height: 200px">
						  <img src="reidbook.PNG" height="158" width="160"></td>
                  <td width="75%" valign="top"> <p><a href="https://www.springer.com/gp/book/9783319409900">
                      <papertitle>Human Re-Identification</papertitle>
                    </a><br>
                    <strong>Ziyan Wu</strong>                    <br>
                    <em>Springer, 
				  2016<br>ISBN 978-3-319-40991-7</em></p><p>This book covers aspects of human re-identification problems related to computer vision and machine learning. Working from a practical perspective, it introduces novel algorithms and designs for human re-identification that bridge the gap between research and reality. The primary focus is on building a robust, reliable, distributed and scalable smart surveillance system that can be deployed in real-world scenarios.  </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%" style="height: 229px">
                    <img src="poseprior.JPG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="http://dx.doi.org/10.1109/TPAMI.2014.2360373">
                        <papertitle>Viewpoint Invariant Human Re-Identification in Camera Networks Using Pose Priors and Subject-Discriminative Features</papertitle></a><br>
                      <strong>Ziyan Wu</strong>, 
					<a href="https://www.linkedin.com/in/yangli625">Yang 
					Li</a>, 
					<span>
					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></span> <br>
                    <span>
                    <em>IEEE Transactions on Pattern Analysis and Machine
                      Intelligence (<strong>TPAMI</strong>),  Vol. 37, No. 5, pp. 1095-1108, May 2015</em>   
                    </span></p>
                 
                    <p>We build a model for human
appearance as a function of pose, using training data gathered from a calibrated camera. We then apply this &#8220;pose prior&#8221; in
online re-identification to make matching and identification more robust to viewpoint. We further integrate person-specific features
learned over the course of tracking to improve the algorithm&#8217;s performance. </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%" style="height: 199px">
                    <img src="LFDA.png" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/li-wacv15.pdf">
                        <papertitle>Multi-Shot Human Re-Identification Using Adaptive Fisher Discriminant Analysis</papertitle></a><br>
                      <a href="https://www.linkedin.com/in/yangli625">Yang 
					Li</a>,<strong> Ziyan Wu</strong>,
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>,&nbsp; 
					<span>
					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></span>&nbsp; <br>
                      <em>British Machine Vision Conference (</em><strong><em>BMVC</em></strong><em>), 
					2015</em><br> </p>
                 
                    <p>We introduce an algorithm to
hierarchically cluster image sequences and use the representative data samples to learn a
feature subspace maximizing the Fisher criterion. The clustering and subspace learning
processes are applied iteratively to obtain diversity-preserving discriminative features.
 </p>
                  </td>
                  </tr>


              	<tr>
                  <td width="25%">
                    <img src="rprf.PNG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/li-wacv15.pdf">
                        <papertitle>Multi-Shot Re-identification with Random-Projection-based Random Forest</papertitle></a><br>
                      <a href="https://www.linkedin.com/in/yangli625">Yang 
					Li</a>, 
                      <strong>Ziyan Wu</strong>,<span><a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> </a>
					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></span> <br>
                      <em>IEEE Winter Conference on Applications of Computer Vision (</em><strong><em>WACV</em></strong><em>), 
					2015</em><p><br>
                 
                    We perform dimensionality reduction on image feature vectors through random projection for multi-shot Re-ID. A random forests is trained based on pairwise constraints in
the projected subspace. During run-time, we select personalized random forests for each subject using their multi-shot appearances.
                 
                    </td>
                  </tr>
				  <tr>
                  <td width="25%">
                    <img src="vi.PNG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="http://www.bmva.org/bmvc/2014/papers/paper090/index.html">
                        <papertitle> Virtual Insertion: Robust Bundle Adjustment over Long Video Sequences</papertitle></a><br>
                      <strong>Ziyan Wu</strong>,<span><a href="https://scholar.google.com/citations?user=a-gkZ9kAAAAJ&amp;hl=en"> 
					<span class="auto-style1">Han-Pang Chiu</span></a></span>, 
					<a href="https://scholar.google.com/citations?hl=en&user=uCZgwJMAAAAJ">
					<span class="auto-style1">Zhiwei Zhu</span></a><br>
                      <em>British Machine Vision Conference (</em><strong><em>BMVC</em></strong><em>), 
			    2014</em> <font color="red"><strong>(oral)</strong></font><br>
                 
                    <p>
					<a href="http://www.bmva.org/bmvc/2014/papers/paper090/index.html">
					talk</a></p>
                    <p>We propose a novel &#8220;virtual insertion&#8221; scheme 
					for Structure from Motion (SfM), which constructs virtual points and virtual frames to adapt the existence of visual landmark link
outage, namely &#8220;visual breaks&#8221; due to no common features observed from neighboring
camera views in challenging environments. </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%">
                    <img src="rpi_thesis.png" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="http://digitool.rpi.edu:8881/R/PLD7F1DG68USTVSN2EXRI98EFUQE4FFYUSJYN7IJDLILKAVU3R-00305?func=dbin-jump-full&object_id=172696&local_base=GEN01&pds_handle=GUEST">
                        <papertitle>Multi-Object Tracking and Association With a Camera Network</papertitle></a><br>
                      <strong>Ziyan Wu</strong><br>
                      <em>Doctoral Dissertation, Rensselaer Polytechnic Institute (<strong>RPI</strong>), 
					2014</em><br>
                 
                    <p>Video surveillance is a critical issue for defense and 
					homeland security applications. There are three key steps of 
					video surveillance: system calibration, multi-object 
					tracking, and target behavior analysis. In this thesis we 
					investigate several important and challenging computer 
					vision problems and applications related to these three 
					steps, in order to improve the performance of video 
					surveillance. </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%" style="height: 212px">
                    <img src="ctflow_prl.PNG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/wu-prl13.pdf">
                        <papertitle>Improving Counterflow Detection in Dense Crowds with Scene Features</papertitle></a><br>
                      <strong>Ziyan Wu</strong>,
					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a><br>
                      <em>Pattern Recognition Letters (<strong>PRL</strong>), Vol. 44, pp. 152-160, July 15, 2014</em></p>
                 
                    <p>This paper addresses the problem of detecting counterflow motion in
videos of highly dense crowds. We focus on improving the detection performance by identifying scene features &#8212; that is, features on motionless
background surfaces. We propose a three-way classifier to differentiate counterflow from normal flow, simultaneously identifying scene features based on
statistics of low-level feature point tracks. </p>
                  </td>
                  </tr>


              	<tr>
                  <td width="25%">
                    <img src="icdsc14.PNG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/radke-icdsc14.pdf">
                        <papertitle>Real-World Re-Identification in an Airport Camera Network</papertitle></a><br>
                      <a href="https://www.linkedin.com/in/yangli625">Yang 
					Li</a>, 
                      <strong>Ziyan Wu</strong>,
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>, 					<a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a><br>
                      <em>ACM/IEEE International Conference on Distributed Smart Cameras (</em><strong><em>ICDSC</em></strong><em>), 
					2014</em><br>
                 
                    <p>We discuss the high-level system design of the video surveillance application, and the issues we encountered during our development and testing. We also describe the algorithm framework for our human re-identification software, and discuss considerations of speed and matching performance.  </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%">
                    <img src="ptz.PNG" height="142" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/wu-pami12.pdf">
                        <papertitle>Keeping a PTZ Camera Calibrated</papertitle></a><br>
                      <strong>Ziyan Wu</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a> <br>
                    <span>
                    <em>IEEE Transactions on Pattern Analysis and Machine
                      Intelligence (<strong>TPAMI</strong>), Vol. 35, No. 8, pp. 1994-2007, 2013</em></span><br>
                 
                    <p> We propose a complete model for a pan-tilt-zoom camera
that explicitly reflects how focal length and lens distortion vary as a function of zoom scale. We show how the parameters of this model
can be quickly and accurately estimated using a series of simple initialization steps followed by a nonlinear optimization. We also show how the calibration parameters can be maintained using
a one-shot dynamic correction process; this ensures that the camera returns the same field of view every time the user requests a given
(pan, tilt, zoom), even after hundreds of hours of operation.  </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%">
                    <img src="cnwasa12.PNG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/wu-cnwasa12.pdf">
                        <papertitle>Using Scene Features to Improve Wide-Area Video Surveillance</papertitle></a><br>
                      <strong>Ziyan Wu</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a> <br>
                      <em>Workshop on Camera Networks and Wide Area Scene Analysis, in conjunction with CVPR (<strong>CVPRW</strong>), 2012</em><br>
                 
                    <p>We introduce two novel methods to improve the performance of wide area video surveillance applications by using scene features. </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%">
                    <img src="airport.PNG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.ecse.rpi.edu/~rjradke/papers/wucnwasa11.pdf">
                        <papertitle>Real-Time Airport Security Checkpoint Surveillance Using a Camera Network</papertitle></a><br>
                      
                      <strong>Ziyan Wu</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a> <br>
                      <em>Workshop on Camera Networks and Wide Area Scene Analysis, in conjunction with CVPR (<strong>CVPRW</strong>), 2011</em><p>
					<a href="https://www.youtube.com/watch?v=BpxGXTcayBs&amp;list=PLuh62Q4Sv7BVld6hKiIjCGUZhdqGiamfv">
					video</a> <br>
                 
                    <p>We introduce an airport security checkpoint surveillance
system using a camera network. The system tracks the
movement of each passenger and carry-on bag, continuously maintains the association between bags and passengers, and verifies that passengers leave the checkpoint with
the correct bags. </p>
                  </td>
                  </tr>


              	<tr>
                  <td width="25%">
                    <img src="icdar11.PNG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.researchgate.net/publication/220861297_Towards_Improved_Paper-Based_Election_Technology">
                        <papertitle> Towards Improved Paper-based Election Technology</papertitle></a><br>
                      <a href="http://coen.boisestate.edu/EBarneySmith/">Elisa Barney 
					Smith</a>, <a href="http://www.cse.lehigh.edu/~lopresti/">Daniel Lopresti</a>, 
					<a href="https://www.ecse.rpi.edu/~nagy/">George Nagy</a>,  <strong>Ziyan Wu </strong> <br>
                      <em>International Conference on Document Analysis and Recognition (<strong>ICDAR</strong>), 2011</em><br>
                 
                    <p>Resources are presented for fostering paper-based election technology. They comprise a diverse collection of real and simulated ballot and survey images, and software tools for ballot synthesis, registration, segmentation, and ground truthing. </p>
                  </td>
                  </tr>
				  <tr>
                  <td width="25%">
                    <img src="drr2011.PNG" height="156" width="156">
                     </td>
                  <td width="75%" valign="top">
                    <p><a href="https://www.researchgate.net/publication/221253858_Characterizing_Challenged_Minnesota_Ballots">
                        <papertitle>Characterizing Challenged Minnesota Ballots</papertitle></a><br>
					<a href="https://www.ecse.rpi.edu/~nagy/">George Nagy</a>,  
					<a href="http://www.cse.lehigh.edu/~lopresti/">Daniel Lopresti</a>, 
					  <a href="http://coen.boisestate.edu/EBarneySmith/">Elisa Barney 
					Smith</a>,  <strong>Ziyan Wu </strong> &nbsp;<br>
                      <em>Document Recognition and Retrieval XVIII (<strong>DRR</strong>), 2011</em><br>
                 
                    <p>Photocopies of the ballots challenged in the 2008 Minnesota elections, which constitute a public record, were scanned on a high-speed scanner and made available on a public radio website. Based on a review of relevant image-processing aspects of paper-based election machinery and on additional statistics and observations on the posted sample data, robust tools were developed for determining the underlying grid of the targets on these ballots regardless of skew, clipping, and other degradations caused by high-speed copying and digitization.  </p>
                  </td>
                  </tr>


              </tbody>
            </table>
            <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
              <tbody>
                <tr>
                  <td> <heading>Synergistic Activities</heading></td>
                </tr>
              </tbody>
            </table>
            <table width="100%" cellpadding="20" border="0" align="center">
              <tbody>
                <tr>
                  <td width="25%"><img src="synergy.PNG" alt="prl" height="160" width="160"></td>
                  <td width="75%" valign="top">
                    <p> Associate Editor, <strong>IEEE Access</strong>, 2017 - present</p>
					<p> Organizer, 
					<a href="https://wvbsd.github.io/2019/index.html">Worshop on Vision with Biased and Scarce 
					Data</a>, in conjunction with <strong>CVPR</strong> 2019</p>
					<p> Organizer, 
					<a href="https://wvbsd.github.io/2018/index.html">Worshop on Vision with Biased and Scarce 
					Data</a>, in conjunction with <strong>CVPR</strong> 2018</p>
					<p> Organizer, 					<a href="http://cvpr2017.thecvf.com/exhibit/spotlights">EXPO 
					Spotlight</a>, <strong>CVPR</strong> 2017</p>
					<p> Organizer, 
					<a href="http://computervisioncentral.com/view/">Vision Industry and Entrepreneur Workshop</a>, in conjunction with <strong>CVPR</strong> 
					2016</p>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
              <tbody>
                <tr>
                  <td> <br>
                  </td>
                </tr>
              </tbody>
            </table>
            <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script>
            <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-12283443-6");
          pageTracker._trackPageview();
          } catch(err) {}
      </script> </td>
        </tr>
      </tbody>
    </table>
    <p class="auto-style2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://jonbarron.info/">Template Credit&nbsp;&nbsp; </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</p>
  </body>
</html>
