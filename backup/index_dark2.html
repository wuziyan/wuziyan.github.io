<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="color-scheme" content="dark">
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/png" href="seal_icon.png">
    <title>Ziyan Wu</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;800&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@700;800;900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  </head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E97EGMN96G"></script>
    <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E97EGMN96G');
</script>
  <body>
    <div class="scroll-progress"></div>
    <a id="top"></a>
    <nav class="site-header">
      <div class="site-header__inner">
        <a class="brand" href="#top">Ziyan Wu</a>
        <button class="mobile-nav-toggle" aria-label="Toggle navigation">
          <i class="fa-solid fa-bars"></i>
        </button>
        <div class="site-nav">
          <a href="#news">News</a>
          <a href="#publications">Publications</a>
          <a href="#synergistic">Synergistic Activities</a>
        </div>
      </div>
    </nav>
    <main class="page">
                    <section class="section hero" id="about">
          <div class="section-inner">
            <div class="hero-card">
              <div class="hero-card__inner">
                <div class="hero-profile">
                  <h1 class="hero-name">Ziyan Wu</h1>
                  <div class="hero-summary">
                    <p>I serve as VP of R&amp;D at <a href="https://www.uii-ai.com/">United Imaging Intelligence (UII America)</a> in Boston, MA, where I lead the creation of computer vision, machine learning, and intelligent robotic systems for medical environments.</p>
                    <p>Previously, I contributed to Siemens Corporate Research in Princeton, NJ and Honeywell Technology Solutions Labs in Shanghai, China.</p>
                    <p>I received my PhD in Computer and Systems Engineering from the Department of Electrical, Computer, and Systems Engineering at <a href="http://www.rpi.edu">Rensselaer Polytechnic Institute</a> in May 2014 under the guidance of <a href="https://www.ecse.rpi.edu/%7Erjradke/index.htm">Prof. Richard J. Radke</a>.</p>
                  </div>
                  <div class="hero-cta-group">
                    <a href="https://scholar.google.com/citations?user=CkPUb-4AAAAJ"><i class="fa-brands fa-google-scholar"></i><span>Google Scholar</span></a>
                    <a href="http://www.linkedin.com/in/wuziyan/"><i class="fa-brands fa-linkedin"></i><span>LinkedIn</span></a>
                    <a href="mailto:ziyan@alum.rpi.edu"><i class="fa-solid fa-envelope"></i><span>Email</span></a>
                  </div>
                </div>
                <div class="hero-avatar">
                  <img src="ziyan_wu.jpg" height="230" width="230" alt="Portrait of Ziyan Wu">
                </div>
              </div>
            </div>
          </div>
        </section>
                    <section class="section" id="openings">
          <div class="section-inner">
            <div class="section-label">
              <h2 class="section-title">Openings</h2>
            </div>
            <div class="section-body">
              <ul class="section-list section-list--highlight section-list--plain">
                <li><strong>[Internship]</strong> We are looking for multiple research interns with computer vision and robotics background to join our Boston team for Spring and Summer 2025. Please email <em>ziyan.wu AT uii-ai DOT com</em> if you are interested.</li>
              </ul>
            </div>
          </div>
        </section>
        <section class="section" id="news">
          <div class="section-inner">
            <div class="section-label">
              <h2 class="section-title">News</h2>
            </div>
            <div class="section-body">
              <ul class="section-list section-list--plain">
                <li>We are organizing the <a href="https://apahws.github.io/">First Workshop on Advanced Perception for Autonomous Healthcare</a> in conjunction with ICCV 2025.</li>
                <li>We organized the <a href="https://fadetrcv.github.io/2025/">Sixth Workshop on Fair, Data Efficient and Trusted Computer Vision</a> in conjunction with CVPR 2025.</li>
                <li>I delivered a talk on "Autonomizing Medical Devices with Visual Perception" at the <a href="https://sites.google.com/view/ai4healthcarefg/">AI4Healthcare Workshop</a> in conjunction with FG 2023.</li>
                <li>We organized the <a href="https://wvbsd.github.io/2022/index.html">Third Workshop on Vision with Biased and Scarce Data</a> in conjunction with ECCV 2022.</li>
                <li>I delivered a talk on "<a href="https://www.buffalo.edu/ai-data-science/news-events/events/iad-days/2022.html">Empowering Medical Scanners with Autonomy</a>" at the <a href="https://www.buffalo.edu/ai-data-science/news-events/events/iad-days/2022.html">IAD Days</a> for the <a href="https://www.buffalo.edu/ai-data-science.html">Institute for Artificial Intelligence and Data Science</a>, University of Buffalo.</li>
              </ul>
            </div>
          </div>
        </section>
        <section class="section" id="research">
          <div class="section-inner">
            <div class="section-label">
              <h2 class="section-title">Research</h2>
            </div>
            <div class="section-body">
              <p class="section-intro">My research explores applied computer vision and machine learning systems that advance intelligent medical imaging and robotics.</p>
              <ul class="section-tags">
                <li>Human pose and shape estimation</li>
                <li>Explainable AI</li>
                <li>Object detection and tracking</li>
                <li>Anomaly detection</li>
                <li>Augmented reality</li>
                <li>Scene understanding</li>
                <li>Human re-identification</li>
                <li>Camera calibration</li>
              </ul>
            </div>
          </div>
        </section>
                                    <section class="section section--publications" id="publications">
          <div class="section-inner">
            <div class="section-label">
              <h2 class="section-title">Publications</h2>
            </div>
            <div class="section-body">
              <div class="pubs-controls" role="toolbar" aria-label="Filter publications">
                <button type="button" class="pubs-chip is-active" data-filter="all">All</button>
                <button type="button" class="pubs-chip" data-filter="highlight">Highlights</button>
                <button type="button" class="pubs-chip" data-filter="2025">2025</button>
                <button type="button" class="pubs-chip" data-filter="2024">2024</button>
                <button type="button" class="pubs-chip" data-filter="earlier">Earlier</button>
              </div>
              <div class="pubs-grid">
          <article class="pub-card" data-year="2025" data-venue="iccv" data-flag="highlight">
            <figure class="pub-card__media">
              <img src="iccv25-1.png" height="180" width="180">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2503.15671">CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=QTqGtPoAAAAJ&hl=en">Arindam Dutta</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, <a href="https://vcg.engr.ucr.edu/amit">Amit K. Roy-Chowdhury</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICCV</span><span class="pub-card__meta-item">2025</span><span class="pub-card__meta-item pub-card__meta-item--flag">Highlight</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2025" data-venue="iccv">
            <figure class="pub-card__media">
              <img src="iccv25-2.png" height="150" width="180">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2503.07946">7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting</a></h3>
            <p class="pub-card__authors"><a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICCV</span><span class="pub-card__meta-item">2025</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We present 7D Gaussian Splatting (7DGS), a unified framework representing scene elements as seven-dimensional Gaussians spanning position (3D), time (1D), and viewing direction (3D). Our key contribution is an efficient conditional slicing mechanism that transforms 7D Gaussians into view- and time-conditioned 3D Gaussians, maintaining compatibility with existing 3D Gaussian Splatting pipelines while enabling joint optimization. Experiments demonstrate that 7DGS outperforms prior methods by up to 7.36 dB in PSNR while achieving real-time rendering (401 FPS) on challenging dynamic scenes with complex view-dependent effects. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2025" data-venue="miccai">
            <figure class="pub-card__media">
              <img src="miccai25.png" height="150" width="180">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2503.24108">PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">MICCAI</span><span class="pub-card__meta-item">2025</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We introduce PolypSegTrack, a novel foundation model that 
            					jointly addresses polyp detection, segmentation, 
            					classification and unsupervised tracking in colonoscopic 
            					videos. Our approach leverages a novel conditional mask 
            					loss, enabling flexible training across datasets with either 
            					pixel-level segmentation masks or bounding box annotations, 
            					allowing us to bypass task-specific fine-tuning. Our 
            					unsupervised tracking module reliably associates polyp 
            					instances across frames using object queries, without 
            					relying on any heuristics. We leverage a robust vision 
            					foundation model backbone that is pre-trained unsupervisedly 
            					on natural images, thereby removing the need for 
            					domain-specific pre-training.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2025" data-venue="cvpr">
            <figure class="pub-card__media">
              <img src="cvpr25.jpg" height="150" width="180">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/pdf/2411.16932">Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding</a></h3>
            <p class="pub-card__authors"><a href="https://dengandong.github.io/">Andong Deng</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, <a href="https://planche.me/">Benjamin Planche</a>,&nbsp;<a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://ukaukaaaa.github.io/">Bin Wang</a>, <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">CVPR</span><span class="pub-card__meta-item">2025</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose
            Seq2Time, a data-oriented training paradigm that leverages sequences of images and short video clips to enhance
            temporal awareness in long videos. By converting sequence
            positions into temporal annotations, we transform largescale image and clip captioning datasets into sequences
            that mimic the temporal structure of long videos, enabling
            self-supervised training with abundant time-sensitive data.
            To enable sequence-to-time knowledge transfer, we introduce a novel time representation that unifies positional information across image sequences, clip sequences, and long
            videos. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2025" data-venue="iclr">
            <figure class="pub-card__media">
              <img src="6dgs.png" height="150" width="180">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2410.04974">6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering</a></h3>
            <p class="pub-card__authors"><a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>,&nbsp;<a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICLR</span><span class="pub-card__meta-item">2025</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>
            					<a href="https://gaozhongpai.github.io/6dgs/">[Project Page]</a><br>
            					</p>
                                <p>Novel view synthesis has advanced significantly with the development of neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However, achieving high quality without compromising real-time rendering remains challenging, particularly for physically-based ray tracing with view-dependent effects.We introduce 6D Gaussian Splatting (6DGS), which enhances color and opacity representations and leverages the additional directional information in the 6D space for optimized Gaussian control. Our approach is fully compatible with the 3DGS framework and significantly improves real-time radiance field rendering by better modeling view-dependent effects and fine details. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2025" data-venue="iclr">
            <figure class="pub-card__media">
              <img src="3dvlgs.png" height="169" width="181">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2410.07577">3D Vision-Language Gaussian Splatting</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=IFqidw4AAAAJ&hl=en">Qucheng Peng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://www.crcv.ucf.edu/chenchen/index.html">Chen Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICLR</span><span class="pub-card__meta-item">2025</span></div>
          </header>
          <div class="pub-card__abstract">
            <p> We propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2025" data-venue="iclr">
            <figure class="pub-card__media">
              <img src="ois.png" height="169" width="178">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2410.12214">Order-aware Interactive Segmentation</a></h3>
            <p class="pub-card__authors"><a href="https://ukaukaaaa.github.io/">Bin Wang</a>, <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://dengandong.github.io/">Andong Deng</a>, <a href="https://sites.google.com/cs.unc.edu/qinliu/home?pli=1">Qin Liu</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://bagcilab.com/people/ulas-bagci-phd/">Ulas Bagci</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICLR</span><span class="pub-card__meta-item">2025</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>
            					<a href="https://ukaukaaaa.github.io/projects/OIS/index.html">
            					[Project Page]</a><br>
            					</p>
                                <p>We introduce a novel order-aware attention, where the order maps seamlessly guide the user interactions (in the form of clicks) to attend to the image features. We further present an object-aware attention module to incorporate a strong object-level understanding to better differentiate objects with similar order. Our approach allows both dense and sparse integration of user clicks, enhancing both accuracy and efficiency as compared to prior works. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2025" data-venue="wacv">
            <figure class="pub-card__media">
              <img src="mrgesture.png" height="169" width="178">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/pdf/2407.14903">Automated Patient Positioning with Learned 3D Hand Gestures</a></h3>
            <p class="pub-card__authors"><a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>*, <a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>*, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">WACV</span><span class="pub-card__meta-item">2025</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose an automated patient positioning system that utilizes a camera to detect specific hand gestures from technicians, allowing users to indicate the target patient region to the system and initiate automated positioning. Our approach relies on a novel multi-stage pipeline to recognize and interpret the technicians&#8217; gestures, translating them into precise motions of medical devices.  </p>
            					<p>*Equal Contributions</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="neurips">
            <figure class="pub-card__media">
              <img src="ddgs.png" height="169" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2406.02518">DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering</a></h3>
            <p class="pub-card__authors"><a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>*, <a href="https://planche.me/">Benjamin Planche</a>*,&nbsp;<a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=DV61tzoAAAAJ&amp;hl=en">Xiao Chen</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">NeurIPS</span><span class="pub-card__meta-item">2024</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We present a novel approach that marries realistic 
            					physics-inspired X-ray simulation with efficient, 
            					differentiable DRR generation using 3D Gaussian splatting 
            					(3DGS). Our direction-disentangled 3DGS (DDGS) method 
            					separates the radiosity contribution into isotropic and 
            					direction-dependent components, approximating complex 
            					anisotropic interactions without intricate runtime 
            					simulations. Additionally, we adapt the 3DGS initialization 
            					to account for tomography data properties, enhancing 
            					accuracy and efficiency. </p>
            					<p>*Equal Contributions</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="eccv">
            <figure class="pub-card__media">
              <img src="eccv24.png" height="150" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2407.09694">Divide and Fuse: Body Part Mesh Recovery from Partially Visible Human Images</a></h3>
            <p class="pub-card__authors"><a href="https://tyluann.github.io/">Tianyu Luan</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://scholar.google.com/citations?user=6DN-P4wAAAAJ&hl=zh-CN">Luyuan Xie</a>, <a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>, <a href="https://arcade.cs.jhu.edu/members/hao-ding.html">Hao Ding</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=GwWK7IoAAAAJ&hl=en">Ange Lou</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ECCV</span><span class="pub-card__meta-item">2024</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We introduce a novel bottom-up approach for human body mesh reconstruction, specifically designed to address the challenges posed by partial visibility and occlusion in input images. 
            					Our method&nbsp; reconstructs human body parts independently before fusing them, thereby ensuring robustness against occlusions. We design 
            					Human Part Parametric Models that independently reconstruct 
            					the mesh from a few shape and global-location parameters, 
            					without inter-part dependency. A specially designed fusion 
            					module then seamlessly integrates the reconstructed parts, 
            					even when only a few are visible. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="miccai" data-flag="highlight">
            <figure class="pub-card__media">
              <img src="miccai24.png" height="150" width="160">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2408.14427">Few-Shot 3D Volumetric Segmentation with Multi-Surrogate Fusion</a></h3>
            <p class="pub-card__authors"><a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a>,<strong> Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">MICCAI</span><span class="pub-card__meta-item">2024</span><span class="pub-card__meta-item pub-card__meta-item--flag">Highlight</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We present MSFSeg, a novel few-shot 3D segmentation framework with a lightweight multi-surrogate fusion (MSF). MSFSeg is able to automatically segment unseen 3D objects/organs (during training) provided with one or a few annotated 2D slices or 3D sequence segments, via learning dense query-support organ/lesion anatomy correlations across patient populations. Our proposed MSF module mines comprehensive and diversified morphology correlations between unlabeled and the few labeled slices/sequences through multiple designated surrogates, making it able to generate accurate cross-domain 3D segmentation masks given annotated slices or sequences. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="mm">
            <figure class="pub-card__media">
              <img src="ccda.png" height="150" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.arxiv.org/abs/2408.02261">Cross-Class Domain Adaptive Semantic Segmentation with Visual Language Models</a></h3>
            <p class="pub-card__authors">Wenqi Ren, <a href="https://github.com/XiaRho/">Ruihao Xia</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <strong>Ziyan Wu</strong>, <a href="http://www.ytangecust.com/">Yang Tang</a>, <a href="https://disi.unitn.it/~sebe/">Nicu Sebe</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">MM</span><span class="pub-card__meta-item">2024</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>This work addresses the issue of cross-class domain adaptation
            (CCDA) in semantic segmentation, where the target domain contains
            both shared and novel classes that are either unlabeled or
            unseen in the source domain. We propose a label alignment
            method by leveraging VLMs to relabel pseudo labels for novel
            classes. We embed a two-stage method to enable fine-grained
            semantic segmentation and design a threshold based on the uncertainty
            of pseudo labels to exclude noisy VLM predictions. To
            further augment the supervision of novel classes, we devise memory
            banks with an adaptive update scheme to effectively manage
            accurate VLM predictions, which are then resampled to increase
            the sampling probability of novel classes. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="cvpr">
            <figure class="pub-card__media">
              <img src="cvpr24.png" height="150" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lou_DaReNeRF_Direction-aware_Representation_for_Dynamic_Scenes_CVPR_2024_paper.pdf">DaReNeRF: Direction-aware Representation for Dynamic Scenes</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=GwWK7IoAAAAJ&hl=en">Ange Lou</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://scholar.google.com/citations?user=lWarbfUAAAAJ&hl=en">Yamin Li</a>, <a href="https://tyluann.github.io/">Tianyu Luan</a>, <a href="https://arcade.cs.jhu.edu/members/hao-ding.html">Hao Ding</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://engineering.vanderbilt.edu/bio/?pid=jack-noble">Jack Noble</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">CVPR</span><span class="pub-card__meta-item">2024</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions. 
                                This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. 
                                DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes. 
                                Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance 
                                in novel view synthesis for complex dynamic scenes. Notably, to address redundancy introduced by the six real and six imaginary 
                                direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="mdpi&nbsp;comput.-sci.-math.-forum,-2024-9(1)">
            <figure class="pub-card__media">
              <img src="aibsd24.png" height="150" width="120">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.mdpi.com/2813-0324/9/1">The 2nd AAAI Workshop on Artificial Intelligence with Biased or Scarce Data (AIBSD)</a></h3>
            <p class="pub-card__authors"><span> <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a>, <a href="https://abhishekaich27.github.io/">Abhishek Aich</a></span>, <strong>Ziyan Wu</strong> (Editors)</p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">MDPI&nbsp;Comput. Sci. Math. Forum, 2024 9(1)</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>The official proceedings of the Second Workshop on 
            					Artificial Intelligence with Biased or Scarce Data in 
            					conjunction with AAAI Conference on Artificial Intelligence 
            					2024.</p>
            					<p>
            					<a href="https://aibsdworkshop.github.io/2024/index.html">
            					workshop website</a></p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="iclr">
            <figure class="pub-card__media">
              <img src="iclr24.png" height="150" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2402.07814">PBADet: A One-Stage Anchor-Free Approach for Part-Body Association</a></h3>
            <p class="pub-card__authors"><a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://scholar.google.com/citations?user=a876WNUAAAAJ">Huayi Zhou</a>, <a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICLR</span><span class="pub-card__meta-item">2024</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. 
                                Building upon the anchor-free object representation across multi-scale feature maps, 
                                we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. 
                                Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="aaai">
            <figure class="pub-card__media">
              <img src="aaai24b.png" height="150" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2312.10246/">Implicit Modeling of Non-rigid Objects with Cross-Category Signals</a></h3>
            <p class="pub-card__authors"><a href="https://www.linkedin.com/in/yuchun-liu">Yuchun Liu</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://www.researchgate.net/profile/Pierre-Sibut-Bourde">Pierre Sibut-Bourde</a>, <a href="https://sites.google.com/a/temple.edu/fan-yang/">Fan Yang</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">AAAI</span><span class="pub-card__meta-item">2024</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>In this work, we propose MODIF, a multi-object deep implicit function that jointly learns the deformation fields and instance-specific latent codes for multiple objects at once. Our emphasis is on non-rigid, non-interpenetrating entities such as organs. To effectively capture the interrelation between these entities and ensure precise, collision-free representations, our approach facilitates signaling between category-specific fields to adequately rectify shapes. We also introduce novel inter-object supervision: an attraction-repulsion loss is formulated to refine contact regions between objects.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="aaai">
            <figure class="pub-card__media">
              <img src="aaai24c.png" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2303.13269/">Disguise without Disruption: Utility-Preserving Face De-Identification</a></h3>
            <p class="pub-card__authors"><a href="https://zikuicai.github.io/">Zikui Cai</a>, <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://intra.ece.ucr.edu/~sasif/">M. Salman Asif</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">AAAI</span><span class="pub-card__meta-item">2024</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>In this paper, we introduce Disguise, a novel algorithm that seamlessly de-identifies facial images while ensuring the usability of the modified data. Unlike previous approaches, our solution is firmly grounded in the domains of differential privacy and ensemble-learning research. Our method involves extracting and substituting depicted identities with synthetic ones, generated using variational mechanisms to maximize obfuscation and non-invertibility. Additionally, we leverage supervision from a mixture-of-experts to disentangle and preserve other utility attributes. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2024" data-venue="aaai">
            <figure class="pub-card__media">
              <img src="aaai24a.png" height="147" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2312.14478">Federated Learning via Input-Output Collaborative Distillation</a></h3>
            <p class="pub-card__authors"><a href="https://gong-xuan.github.io/">Xuan Gong</a>, <a href="https://scholar.google.com/citations?user=4N1hFycAAAAJ">Shanglin Li</a>, <a href="https://openreview.net/profile?id=~Yuxiang_Bao1">Yuxiang Bao</a>, <a href="https://barry-yao.netlify.app/">Barry Yao</a>, <a href="https://yawen-hwang.github.io/">Yawen Huang</a>, <strong>Ziyan Wu</strong>, <a href="https://scholar.google.com/citations?user=ImJz6MsAAAAJ">Baochang Zhang</a>, <a href="https://scholar.google.com/citations?user=vAIECxgAAAAJ">Yefeng Zheng</a>, <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">AAAI</span><span class="pub-card__meta-item">2024</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a federated learning framework eliminating any requirement of recursive local parameter exchange or auxiliary task-relevant data to transfer knowledge, thereby giving direct privacy control to local users. In particular, to cope with the inherent data heterogeneity across locals, our technique learns to distill input on which each local model produces consensual yet unique results to represent each expertise.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2023" data-venue="iccv" data-flag="highlight">
            <figure class="pub-card__media">
              <img src="iccv23.png" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xia_CMDA_Cross-Modality_Domain_Adaptation_for_Nighttime_Semantic_Segmentation_ICCV_2023_paper.html/">CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation</a></h3>
            <p class="pub-card__authors"><a href="https://github.com/XiaRho/">Ruihao Xia</a>, <a href="https://zxcqlf.github.io/">Chaoqiang Zhao</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <strong>Ziyan Wu</strong>, <a href="https://scholar.google.com/citations?user=HS2WuHkAAAAJ">Qiyu Sun</a>, <a href="http://www.ytangecust.com/">Yang Tang</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICCV</span><span class="pub-card__meta-item">2023</span><span class="pub-card__meta-item pub-card__meta-item--flag">Highlight</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>
            					[<a href="https://github.com/XiaRho/CMDA">Project Page</a>]<br>
            					</p>
            
                                <p>Event cameras, as a new form of vision sensors, are complementary to conventional cameras with their high dynamic range. To this end, we propose a novel unsupervised Cross-Modality Domain Adaptation (CMDA) framework to leverage multi-modality (Images and Events) information for nighttime semantic segmentation, with only labels on daytime images.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2023" data-venue="aaai" data-flag="highlight">
            <figure class="pub-card__media">
              <img src="aaai23.png" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25144">Progressive Multi-view Human Mesh Recovery with Self-Supervision</a></h3>
            <p class="pub-card__authors"><a href="https://gong-xuan.github.io/">Xuan Gong</a>, <a href="https://lsongx.github.io/">Liangchen Song</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>,&nbsp; <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>, <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">AAAI</span><span class="pub-card__meta-item">2023</span><span class="pub-card__meta-item pub-card__meta-item--flag">Highlight</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a novel simulation-based training pipeline for multi-view human mesh recovery, which (a) relies on intermediate 2D representations which are more robust to synthetic-to-real domain gap; (b) leverages learnable calibration and triangulation to adapt to more diversified camera setups; and (c) progressively aggregates multi-view information in a canonical 3D space to remove ambiguities in 2D representations. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="tmi">
            <figure class="pub-card__media">
              <img src="TMI2022.jpg" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2210.08464">Federated Learning with Privacy-Preserving Ensemble Attention Distillation</a></h3>
            <p class="pub-card__authors"><a href="https://gong-xuan.github.io/">Xuan Gong</a>, <a href="https://lsongx.github.io/">Liangchen Song</a>, <a href="https://www.linkedin.com/in/rishivedula">Rishi Vedula</a>, <a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>,&nbsp; <a href="https://www.linkedin.com/in/arun-innanje">Arun Innanje</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>, <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">TMI</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a privacy-preserving FL framework leveraging unlabeled public data for one-way offline knowledge distillation in this work. The central model is learned from local knowledge via ensemble attention distillation. Our technique uses decentralized and heterogeneous local data like existing FL approaches, but more importantly, it significantly reduces the risk of privacy leakage. We demonstrate that our method achieves very competitive performance with more robust privacy preservation based on extensive experiments on image classification, segmentation, and reconstruction tasks.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="neurips" data-flag="highlight">
            <figure class="pub-card__media">
              <img src="SHENet.PNG" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2210.08732">Forecasting Human Trajectory from Scene History</a></h3>
            <p class="pub-card__authors"><a href="https://www.linkedin.com/in/%E6%BB%A1%E6%88%90-%E5%AD%9F-823010161">Mancheng Meng</a>, <strong>Ziyan Wu</strong>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://scholar.google.com/citations?user=O5fKAhoAAAAJ&hl=en">Xiran Cai</a>, <a href="https://scholar.google.com/citations?user=-bp44DoAAAAJ&hl=en">Xiang Sean Zhou</a>, <a href="https://sites.google.com/a/temple.edu/fan-yang/">Fan Yang</a>, <span> <a href="https://scholar.google.com/citations?user=v6VYQC8AAAAJ&hl=en"> Dinggang Shen</a></span></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">NeurIPS</span><span class="pub-card__meta-item">2022</span><span class="pub-card__meta-item pub-card__meta-item--flag">Highlight</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>
            					&nbsp;[<a href="https://github.com/MaKaRuiNah/SHENet">Project Page</a>]<br>
            					</p>
                                <p>The moving patterns of human in a constrained scenario 
            					typically conform to a limited number of regularities to a 
            					certain extent, because of the scenario restrictions and 
            					person-person or person-object interactivity. We propose to forecast a person's future 
            					trajectory by learning from the implicit scene regularities. 
            					We call the regularities, inherently derived from the past 
            					dynamics of the people and the environment in the scene, 
            					scene history.&nbsp; We introduce a novel framework 
            					Scene History Excavating Network (SHENet), where the scene 
            					history is leveraged in a simple yet effective approach.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="eccv" data-flag="highlight">
            <figure class="pub-card__media">
              <img src="pref.PNG" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/pdf/2209.10691.pdf">PREF: Predictability Regularized Neural Motion Fields</a></h3>
            <p class="pub-card__authors"><a href="https://lsongx.github.io/">Liangchen Song</a>, <a href="https://gong-xuan.github.io/">Xuan Gong</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>, <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ECCV</span><span class="pub-card__meta-item">2022</span><span class="pub-card__meta-item pub-card__meta-item--flag">Highlight</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>
            					&nbsp;[<a href="http://pref.uiius.com/">Project Page</a>]<br>
            					</p>
                                <p>We leverage a neural motion field for estimating the motion of all points in a multiview setting. Modeling the motion from a dynamic scene with multiview data is challenging due to the ambiguities in points of similar color and points with time-varying color. We propose to regularize the estimated motion to be predictable. If the motion from previous frames is known, then the motion in the near future should be predictable. Therefore, we introduce a predictability regularization by first conditioning the estimated motion on latent embeddings, then by adopting a predictor network to enforce predictability on the embeddings.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="eccv">
            <figure class="pub-card__media">
              <img src="cra.PNG" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/pdf/2209.04596.pdf">Self-supervised Human Mesh Recovery with Cross-Representation Alignment</a></h3>
            <p class="pub-card__authors"><a href="https://gong-xuan.github.io/">Xuan Gong</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ECCV</span><span class="pub-card__meta-item">2022</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose cross-representation alignment utilizing the complementary information from the robust but sparse representation (2D keypoints). Specifically, the alignment errors between initial mesh estimation and both 2D representations are forwarded into regressor and dynamically corrected in the following mesh regression. This adaptive cross-representation alignment explicitly learns from the deviations and captures complementary information: robustness from sparse representation and richness from dense representation. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="eccv">
            <figure class="pub-card__media">
              <img src="pseudoclick.PNG" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/pdf/2207.05282.pdf">PseudoClick: Interactive Image Segmentation with Click Imitation</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=o209D9kAAAAJ&hl=en">Qin Liu</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://biag.cs.unc.edu/">Marc Niethammer</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ECCV</span><span class="pub-card__meta-item">2022</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We ask the question: can our model directly predict where to click, so as to further reduce the user interaction cost? To this end, we propose PseudoClick, a 
                                generic framework that enables existing segmentation networks to propose candidate next clicks. These automatically generated clicks, termed pseudo clicks in this work, 
                                serve as an imitation of human clicks to refine the 
                                segmentation mask. We build PseudoClick on existing segmentation backbones and show how our click prediction mechanism leads to improved performance.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="miccai" data-flag="highlight">
            <figure class="pub-card__media">
              <img src="MICCAI22.JPG" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://link.springer.com/chapter/10.1007/978-3-031-16449-1_12">Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion</a></h3>
            <p class="pub-card__authors"><a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://gong-xuan.github.io/">Xuan Gong</a>, <a href="https://sites.google.com/a/temple.edu/fan-yang/">Fan Yang</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">MICCAI</span><span class="pub-card__meta-item">2022</span><span class="pub-card__meta-item pub-card__meta-item--flag">Highlight</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a generic modularized 3D patient modeling 
            					method consists of (a) a multi-modal keypoint detection 
            					module with attentive fusion for 2D patient joint 
            					localization, to learn complementary cross-modality patient 
            					body information, leading to improved keypoint localization 
            					robustness and generalizability in a wide variety of imaging and clinical scenarios; and (b) a self-supervised 3D mesh regression module which does not require expensive 3D mesh parameter annotations to train, bringing immediate cost benefits for clinical deployment.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="ijcai">
            <figure class="pub-card__media">
              <img src="similarity.jpg" height="141" width="158">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/1911.07381">Visual Similarity Attention</a></h3>
            <p class="pub-card__authors"><a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a> <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">IJCAI</span><span class="pub-card__meta-item">2022</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose the first method to generate generic visual 
            					similarity explanations with gradient-based attention. We 
            					demonstrate that our technique is agnostic to the specific 
            					similarity model type, e.g., we show applicability to 
            					Siamese, triplet, and quadruplet models. Furthermore, we 
            					make our proposed similarity attention a principled part of 
            					the learning process, resulting in a new paradigm for 
            					learning similarity functions. We demonstrate that our 
            					learning mechanism results in more generalizable, as well as 
            					explainable, similarity models.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="cvpr">
            <figure class="pub-card__media">
              <img src="sliding_axial_view.gif" height="138" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_SMPL-A_Modeling_Person-Specific_Deformable_Anatomy_CVPR_2022_paper.pdf">SMPL-A: Modeling Person-Specific Deformable Anatomy</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=MiOqrWkAAAAJ">Hengtao Guo</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">CVPR</span><span class="pub-card__meta-item">2022</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We present the first learning-based approach to estimate the patient&#8217;s internal organ deformation for arbitrary human poses in order to assist with radiotherapy and similar medical protocols. The underlying method first leverages medical scans to learn a patient-specific representation that potentially encodes the organ&#8217;s shape and elastic properties. During inference, given the patient&#8217;s current body pose information and the organ's representation extracted from previous medical scans, our method can estimate their current organ deformation to offer guidance to clinicians.  </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="mdpi&nbsp;comput.-sci.-math.-forum,-2022-3(1)">
            <figure class="pub-card__media">
              &nbsp;&nbsp;&nbsp;&nbsp;
                                  <img src="aibsd.jpg" height="147" width="97">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.mdpi.com/2813-0324/3/1">AAAI Workshop on Artificial Intelligence with Biased or Scarce Data (AIBSD)</a></h3>
            <p class="pub-card__authors"><span> <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a></span>, <strong>Ziyan Wu</strong> (Editors)</p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">MDPI&nbsp;Comput. Sci. Math. Forum, 2022 3(1)</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>The official proceedings of the First Workshop on 
            					Artificial Intelligence with Biased or Scarce Data in 
            					conjunction with AAAI Conference on Artificial Intelligence 
            					2022.</p>
            					<p>
            					<a href="https://aibsdworkshop.github.io/2022/index.html">
            					workshop website</a></p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="aaai">
            <figure class="pub-card__media">
              <img src="aaai22.png" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.aaai.org/AAAI22Papers/AISI-1420.GongX.pdf">Preserving Privacy in Federated Learning with Ensemble Cross-Domain Knowledge Distillation</a></h3>
            <p class="pub-card__authors"><a href="https://gong-xuan.github.io/">Xuan Gong</a>, <a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <strong>Ziyan Wu</strong>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>, <a href="https://www.linkedin.com/in/arun-innanje">Arun Innanje</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">AAAI</span><span class="pub-card__meta-item">2022</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a quantized and noisy ensemble of local predictions from completely trained local models for stronger privacy guarantees without sacrificing accuracy. Based on extensive experiments on classification and segmentation tasks, we show that our method outperforms baseline FL algorithms with superior performance in both accuracy and data privacy preservation.
             </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2022" data-venue="wacv">
            <figure class="pub-card__media">
              <img src="wacv22.PNG" height="143" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Multi-Motion_and_Appearance_Self-Supervised_Moving_Object_Detection_WACV_2022_paper.pdf">Multi-motion and Appearance Self-Supervised Moving Object Detection</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=4UxzoMkAAAAJ&hl=en">Fan Yang</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">WACV</span><span class="pub-card__meta-item">2022</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a Multi-motion and Appearance Self-supervised Network (MASNet) to introduce multi-scale motion information and appearance information of scene for MOD. 
            					Introducing multi-scale motion can aggregate these regions 
            					to form a more complete detection. Appearance information 
            					can serve as another cue for MOD when the motion 
            					independence is not reliable and for removing false 
            					detection in background caused by locally independent 
            					background motion. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2021" data-venue="bmvc">
            <figure class="pub-card__media">
              <img src="omr.png" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.bmvc2021-virtualconference.com/assets/papers/0487.pdf">Everybody Is Unique: Towards Unbiased Human Mesh Recovery</a></h3>
            <p class="pub-card__authors"><a href="https://liren2515.github.io/page/">Ren Li</a>, <span> <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, </span><strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">BMVC</span><span class="pub-card__meta-item">2021</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We present a generalized human mesh optimization algorithm that substantially improves the performance of existing methods on both obese person images as well as community-standard benchmark datasets. 
            					The proposed method utilizes only 2D annotations without 
            					relying on supervision from expensive-to-create mesh 
            					parameters.  </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="international-conference-on-3d-vision-(">
            <figure class="pub-card__media">
              <img src="3dv21.PNG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/2107.12847">Learning Local Recurrent Models for Human Mesh Recovery</a></h3>
            <p class="pub-card__authors"><a href="http://bragilee.github.io/">Runze Li</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://liren2515.github.io/page/">Ren Li</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://vislab.ucr.edu/PEOPLE/BIR_BHANU/index.php"> Bir Bhanu</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">International Conference on 3D Vision (</span></div>
          </header>
          <div class="pub-card__abstract">
            We present a new method for video mesh recovery that divides the human mesh into several local parts following the standard skeletal model. We then model the dynamics of each local part with separate recurrent models, with each model conditioned appropriately based on the known kinematic structure of the human body.  </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2021" data-venue="iccv">
            <figure class="pub-card__media">
              <img src="FedDA.png" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gong_Ensemble_Attention_Distillation_for_Privacy-Preserving_Federated_Learning_ICCV_2021_paper.pdf">Ensemble Attention Distillation for Privacy-Preserving Federated Learning</a></h3>
            <p class="pub-card__authors"><a href="https://gong-xuan.github.io/">Xuan Gong</a>, <a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <strong>Ziyan Wu</strong>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>, <a href="https://www.linkedin.com/in/arun-innanje">Arun Innanje</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICCV</span><span class="pub-card__meta-item">2021</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a new distillation-based FL framework that can 
            					preserve privacy by design, while also consuming 
            					substantially less network communication resources when 
            					compared to the current methods. Our framework engages in 
            					inter-node communication using only publicly available and 
            					approved datasets, thereby giving explicit privacy control 
            					to the user. To distill knowledge among the various local 
            					models, our framework involves a novel ensemble distillation 
            					algorithm that uses both final prediction as well as model 
            					attention.
             </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2021" data-venue="iccv">
            <figure class="pub-card__media">
              <img src="STreid.png" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/pdf/2107.11878.pdf">Spatio-Temporal Representation Factorization for Video-based Person Re-Identification</a></h3>
            <p class="pub-card__authors"><a href="https://abhishekaich27.github.io/">Abhishek Aich</a>, <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://vcg.engr.ucr.edu/amit">Amit K. Roy-Chowdhury</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICCV</span><span class="pub-card__meta-item">2021</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose Spatio-Temporal Representation Factorization 
            					(STRF), a flexible new computational unit that can be used 
            					in conjunction with most existing 3D convolutional neural 
            					network architectures for re-ID. The key innovations of STRF 
            					over prior work include explicit pathways for learning 
            					discriminative temporal and spatial features, with each 
            					component further factorized to capture complementary 
            					person-specific appearance and motion information. 
            					Specifically, temporal factorization comprises two branches, 
            					one each for static features (e.g., the color of clothes) 
            					that do not change much over time, and dynamic features 
            					(e.g., walking patterns) that change over time. 
             </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2021" data-venue="cvpr">
            <figure class="pub-card__media">
              <img src="vrx.JPG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/pdf/2105.00290.pdf">A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</a></h3>
            <p class="pub-card__authors"><a href="https://gyhandy.github.io/">Yunhao Ge</a>, <a href="https://avaxiao.github.io/">Yao Xiao</a>, <a href="https://scholar.google.com/citations?user=ngO9JHUAAAAJ&amp;hl=en">Zhi Xu</a>, <a href="http://homepages.rpi.edu/~zhengm3/">Meng Zheng</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="http://ilab.usc.edu/itti/">Laurent Itti</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">CVPR</span><span class="pub-card__meta-item">2021</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a novel framework to interpret neural networks which extracts relevant class-specific visual concepts and organizes them using structural concepts graphs based on pairwise concept relationships. By means of knowledge distillation,
            we show VRX can take a step towards mimicking the reasoning process of NNs and provide logical, concept-level explanations for final model decisions. With extensive experiments, we empirically show VRX can meaningfully answer &#8220;"why&#8221; and &#8220;"why not&#8221; questions about the prediction,
            providing easy-to-understand insights about the reasoning process. We also show that these insights can potentially provide guidance on improving NN&#8217;s performance.
             </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="tpami">
            <figure class="pub-card__media">
              <img src="zdda_tpami21.JPG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://ieeexplore.ieee.org/document/9361131">Zero-shot Deep Domain Adaptation with Common Representation Learning</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=HkKr0RMAAAAJ&amp;hl=en"> Mohammed Kutbi</a>, <span> <a href="https://scholar.google.com/citations?user=GBi3LYkAAAAJ&hl=en">Kuan-Chuan Peng</a></span>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">TPAMI</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We proposed zero-shot deep domain adaptation (ZDDA). ZDDA-C/ML learns to generate common representations for 
            					source and target domains data. Then, either domain 
            					representation is used later to train a system that works on 
            					both domains or having the ability to eliminate the need to 
            					either domain in sensor fusion settings. In this paper, two variants of 
            					ZDDA have been developed for classification and metric learning task respectively. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="tmi">
            <figure class="pub-card__media">
              <img src="TMI2020-2.JPG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://ieeexplore.ieee.org/document/9284467">Learning Hierarchical Attention for Weakly-supervised Chest X-Ray Abnormality Localization and Diagnosis</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=bCvdh54AAAAJ&hl=en">Xi Ouyang</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <strong>Ziyan Wu</strong>&#65292;<a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://scholar.google.com/citations?user=mtnWns8AAAAJ&hl=zh-CN">Jiayu Huo</a>, <a href="https://scholar.google.com/citations?user=-bp44DoAAAAJ&hl=en">Xiang Sean Zhou</a>, <a href="https://scholar.google.com/citations?user=m6ZNDewAAAAJ&hl=en">Qian Wang</a>, <a href="https://scholar.google.com/citations?user=tSpNhZcAAAAJ&hl=en">Jie-Zhi Cheng</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">TMI</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a new attention-driven weakly supervised algorithm comprising a hierarchical attention mining framework that unifies activation- and gradient-based visual attention in a holistic manner. Our key algorithmic innovations include the design of explicit ordinal attention constraints, enabling principled model training in a weakly-supervised fashion, while also facilitating the generation of visual-attention-driven model explanations by means of localization cues.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2020" data-venue="miccai">
            <figure class="pub-card__media">
              <img src="MICCAI2020.JPG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://link.springer.com/chapter/10.1007/978-3-030-59716-0_9">Robust Multi-modal 3D Patient Body Modeling</a></h3>
            <p class="pub-card__authors"><a href="https://sites.google.com/a/temple.edu/fan-yang/">Fan Yang</a>*, <a href="https://scholar.google.com/citations?user=bCvdh54AAAAJ&hl=en">Ren Li</a>*, <a href="https://cs.gmu.edu/~ggeorgak/">Georgios Georgakis</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a>, <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">MICCAI</span><span class="pub-card__meta-item">2020</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>This paper considers the problem of 3D patient body 
            					modeling. Such a 3D model provides valuable information for 
            					improving patient care, streamlining clinical workflow, 
            					automated parameter optimization for medical devices etc. We 
            					present a novel robust dynamic fusion technique that 
            					facilitates flexible multi-modal inference, resulting in 
            					accurate 3D body modeling even when the input sensor 
            					modality is only a subset of the training modalities.</p>
            					<p>*Equal Contributions</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2020" data-venue="eccv">
            <figure class="pub-card__media">
              <img src="HKMR.JPG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620749.pdf">Hierarchical 
					Kinematic Human Mesh Recovery</a></h3>
            <p class="pub-card__authors"><a href="https://cs.gmu.edu/~ggeorgak/">Georgios Georgakis</a>*, <a href="https://liren2515.github.io/page/">Ren Li</a>*, <span> <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, <a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a>,<strong> </strong> </span> <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ECCV</span><span class="pub-card__meta-item">2020</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>In this work, we address this gap by proposing a new 
            					technique for regression of human parametric model that is 
            					explicitly informed by the known hierarchical structure, 
            					including joint interdependencies of the model. This results 
            					in a strong prior-informed design of the regressor 
            					architecture and an associated hierarchical optimization 
            					that is flexible to be used in conjunction with the current 
            					standard frameworks for 3D human mesh recovery. </p>
            					<p>*Equal Contributions</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="tmi">
            <figure class="pub-card__media">
              <img src="TMI2020-1.JPG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://ieeexplore.ieee.org/document/9084097">Towards Contactless Patient Positioning</a></h3>
            <p class="pub-card__authors"><a href="https://sites.google.com/a/temple.edu/fan-yang/">Fan Yang</a>*, <a href="https://karanams.github.io/">Srikrishna Karanam</a>*, <a href="https://liren2515.github.io/page/">Ren Li</a>*, <a href="https://www.researchgate.net/profile/Wei_Hu15">Wei Hu</a>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>,<span><strong> </strong> </span> <strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">TMI</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>
            					&nbsp;<a href="https://www.youtube.com/watch?v=TuKuqBVpSZM">MICCAI webinar talk</a><br>
            					</p>
                                <p>The COVID-19 pandemic, caused by the highly contagious 
            					SARS-CoV-2 virus, has overwhelmed healthcare systems 
            					worldwide, putting medical professionals at a high risk of 
            					getting infected themselves due to a global shortage of 
            					personal protective equipment. To help alleviate this 
            					problem, we design and develop a contactless patient 
            					positioning system that can enable scanning patients in a 
            					completely remote and contactless fashion. Our key design 
            					objective is to reduce the physical contact time with a 
            					patient as much as possible, which we achieve with our 
            					contactless workflow.</p>
            					<p>*Equal Contributions</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="rbme">
            <figure class="pub-card__media">
              <img src="RBME2020.png" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9069255">Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation and Diagnosis for COVID-19</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=MztESj8AAAAJ&hl=en"><span class="auto-style1">Feng Shi</span></a>*, <a href="https://scholar.google.com/citations?user=dpMPbHsAAAAJ&hl=en">Jun Wang</a>*, <a href="https://scholar.google.com/citations?user=GLF3Pl8AAAAJ&hl=en">Jun Shi</a>*, <strong>Ziyan Wu</strong>, <a href="https://scholar.google.com/citations?user=m6ZNDewAAAAJ&hl=en">Qian Wang,</a> <a href="https://royjames.github.io/zhy/">Zhenyu Tang</a>, <span> <a href="https://scholar.google.com/citations?user=0Do_BMIAAAAJ&hl=en"> Kelei He</a>, <span> <a href="https://scholar.google.com/citations?user=m6BKDUMAAAAJ&hl=en"> Yinghuan Shi</a> ,<span> <a href="https://scholar.google.com/citations?user=v6VYQC8AAAAJ&hl=en"> Dinggang Shen</a></span></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">RBME</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We cover the entire pipeline of medical imaging and analysis techniques involved with COVID-19, including image acquisition, segmentation, diagnosis, and follow-up. We particularly focus on the integration of AI with X-ray and CT, both of which are widely used in the frontline hospitals, in order to depict the latest progress of medical imaging and radiology fighting against COVID-19. </p>
            					<p>*Equal Contributions</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2020" data-venue="cvpr">
            <figure class="pub-card__media">
              <img src="evae.JPG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/1911.07389">Towards Visually Explaining Variational Autoencoders</a></h3>
            <p class="pub-card__authors"><a href="https://scholar.google.com/citations?user=6yh4f4YAAAAJ&hl=en"><span class="auto-style1">Wenqian Liu</span></a>*, <a href="https://scholar.google.com/citations?user=bCvdh54AAAAJ&hl=en">Runze Li</a>*, <a href="http://homepages.rpi.edu/~zhengm3/">Meng Zheng</a>, <span> <a href="https://karanams.github.io/">Srikrishna Karanam</a><strong>, </strong> </span> <strong>Ziyan Wu</strong>, <a href="https://vislab.ucr.edu/PEOPLE/BIR_BHANU/index.php">Bir Bhanu,</a> <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a>, <span> <a href="http://www.ece.neu.edu/people/camps-octavia"> Octavia Camps</a></span></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">CVPR</span><span class="pub-card__meta-item">2020</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose the first technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, 
            					and how they can be infused into model training, helping 
            					bootstrap the VAE into learning improved latent space 
            					disentanglement.
             </p>
            					<p>*Equal Contributions</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2019" data-venue="neurips">
            <figure class="pub-card__media">
              <img src="iss.JPG" height="157" width="157">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/1811.12297">Incremental Scene Synthesis</a></h3>
            <p class="pub-card__authors"><a href="http://planche.me/">Benjamin Planche</a>, <a href="https://xrong.org/">Xuejian Rong</a>, <strong>Ziyan Wu</strong>, <span> <a href="https://karanams.github.io/">Srikrishna Karanam</a>,<a href="http://www.fim.uni-passau.de/en/distributed-information-systems/">Harald Kosch</a>,&nbsp;<a href="http://scholar.google.com/citations?user=aAWeB4wAAAAJ&amp;hl=en">YingLi Tian</a>,<a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> Jan Ernst</a>, </span> <a href="https://www.researchgate.net/profile/Andreas_Hutter">Andreas Hutter</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">NeurIPS</span><span class="pub-card__meta-item">2019</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We present a method to incrementally generate complete 2D or 3D scenes with global consistentcy at each step according to a
            learned scene prior. Real observations of a scene can be incorporated while observing global consistency and unobserved regions can be hallucinated locally in
            consistence with previous observations, hallucinations as well as global priors. Hallucinations are statistical in nature, i.e., different scenes can be generated from
            the same observations. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2019" data-venue="iccv">
            <figure class="pub-card__media">
              <img src="ICASC.png" height="160" width="160">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/1811.07484">Sharpen Focus: Learning with Attention Separability and Consistency</a></h3>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICCV</span><span class="pub-card__meta-item">2019</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We improve the generalizability of CNNs by means of a new framework that makes class-discriminative attention a principled part of the learning process. We propose new learning objectives for attention separability and cross-layer consistency, which result in improved attention discriminability and reduced visual
            confusion. </p>
          </div>
            </div>
          </article>
          <article class="pub-card">
            <figure class="pub-card__media">
              <img src="RGB2CAD.png" height="160" width="160">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://arxiv.org/abs/1811.07249">Learning Local RGB-to-CAD Correspondences for Object Pose Estimation</a></h3>
            <p class="pub-card__authors"><a href="https://cs.gmu.edu/~ggeorgak/">Georgios Georgakis</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>,</p>
          </header>
          <div class="pub-card__abstract">
            <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019 <br>
                                <p>We solve the key problem of existing 3D object pose estimation methods requiring expensive 3D pose annotations by proposing a new method that matches RGB images to CAD models for object pose estimation. 
                                Our method requires neither real-world textures for CAD models nor explicit 3D pose annotations for RGB images.</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="tpami">
            <figure class="pub-card__media">
              <img src="benchmark_pami19.JPG" height="132" width="161">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-pami18.pdf">A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets</a></h3>
            <p class="pub-card__authors"><span> <a href="https://karanams.github.io/">Srikrishna Karanam</a>*, <a href="https://neu-gou.github.io/">Mengran Gou</a>*, <strong>Ziyan Wu</strong>, Angels Rates-Borras, <a href="http://www.ece.neu.edu/people/camps-octavia"> Octavia Camps</a>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">TPAMI</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>
            
            					<a href="https://arxiv.org/abs/1605.09653">supplementary</a> 
            					/
            					<a href="http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/">
            					dataset</a> / 
            					<a href="https://github.com/RSL-NEU/person-reid-benchmark">
            					code</a>
                                </p>
                                <p>We present an extensive review and
            performance evaluation of single and multi-shot re-id algorithms. The experimental protocol incorporates 11 feature extraction 
            					and 22 metric learning and ranking techniques and evaluates
            using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 16 other publicly available datasets. </p>
            					<p>*Equal Contributions</p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-year="2018" data-venue="eccv">
            <figure class="pub-card__media">
              <img src="zdda_eccv18.JPG" height="132" width="161">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Kuan-Chuan_Peng_Zero-Shot_Deep_Domain_ECCV_2018_paper.pdf">Zero Shot Deep Domain Adaptation</a></h3>
            <p class="pub-card__authors"><span> <a href="http://chenlab.ece.cornell.edu/people/kuanchuan/">Kuan-Chuan Peng</a></span>, <strong>Ziyan Wu</strong>, <span> <a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en">Jan Ernst</a></span></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ECCV</span><span class="pub-card__meta-item">2018</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose zero-shot deep domain adaptation (ZDDA) for 
            					domain adaptation and sensor fusion. ZDDA learns from the 
            					task-irrelevant dual-domain pairs when the task-relevant 
            					target-domain training data is unavailable. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="ieee-transactions-on-circuits-and-systems-for-video-technology-(">
            <figure class="pub-card__media">
              <img src="affinehull.png" height="160" width="160">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-tcsvt17.pdf">Learning Affine Hull Representations for Multi-Shot Person Re-Identification</a></h3>
            <p class="pub-card__authors"><a href="https://karanams.github.io/">Srikrishna Karanam</a>, <strong>Ziyan Wu</strong>,<span><a href="http://scholar.google.com/citations?user=hFUJkFgAAAAJ&amp;hl=en"> </a></span> <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">IEEE Transactions on Circuits and Systems for Video Technology (</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="journal-of-medical-imaging-(">
            <figure class="pub-card__media">
              <img src="vessel.png" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ncbi.nlm.nih.gov/pubmed/28413808">Vessel Tree Tracking in Angiographic Sequences</a></h3>
            <p class="pub-card__authors"><a href="http://www.dromston.com/">Dong Zhang</a>, <span> <a href="https://sites.google.com/site/shanhuisun">Shanhui Sun</a></span>, <strong>Ziyan Wu</strong>, <a href="https://sites.google.com/site/borjengchen/"> Bor-Jeng Chen</a>, <span> <a href="http://www.ifp.illinois.edu/~tchen5/AboutMe.html"> Terrence Chen</a></span></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">Journal of Medical Imaging (</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="ieee-transactions-on-circuits-and-systems-for-video-technology-(">
            <figure class="pub-card__media">
              <img src="lab2realworld.JPG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/radke-csvt16.pdf">From the Lab to the Real World: Re-Identification in an Airport Camera Network</a></h3>
            <p class="pub-card__authors"><span> <a href="http://www.ece.neu.edu/people/camps-octavia"> Octavia Camps</a></span>, <span> <a href="https://neu-gou.github.io/">Mengran Gou</a></span>, <a href="https://linkedin.com/in/tom-hebble-8b625b55">Tom Hebble</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://scholar.google.com/citations?user=dkK56rkAAAAJ&amp;hl=en">Oliver Lehmann</a>, <a href="https://www.linkedin.com/in/yangli625">Yang Li</a>, <span> <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></span>, <strong>Ziyan Wu</strong>, <a href="https://scholar.google.com/citations?user=zs3rtqcAAAAJ&amp;hl=en">Fei Xiong </a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">IEEE Transactions on Circuits and Systems for Video Technology (</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="ieee-international-symposium-on-biomedical-imaging-(">
            <figure class="pub-card__media">
              <img src="guidewire.jpg" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://ieeexplore.ieee.org/document/7493221">Guidewire Tracking Using a Novel Sequential Segment Optimization Method in Interventional X-Ray Videos</a></h3>
            <p class="pub-card__authors"><a href="https://sites.google.com/site/borjengchen/"> Bor-Jeng Chen</a>, <strong>Ziyan Wu</strong>, <span> <a href="https://sites.google.com/site/shanhuisun">Shanhui Sun</a></span>, <a href="http://www.dromston.com/">Dong Zhang</a>, <span> <a href="http://www.ifp.illinois.edu/~tchen5/AboutMe.html"> Terrence Chen</a></span></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">IEEE International Symposium on Biomedical Imaging (</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="springer,-2016isbn-978-3-319-40991-7">
            <figure class="pub-card__media">
              <img src="reidbook.PNG" height="158" width="160">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.springer.com/gp/book/9783319409900">Human Re-Identification</a></h3>
            <div class="pub-card__meta"><span class="pub-card__meta-item">Springer, 2016ISBN 978-3-319-40991-7</span></div>
          </header>
          <div class="pub-card__abstract">
            <p><a class="auto-style6" href="amazon.com/author/ziyan">
            				  Amazon Author Page</a></p>
            				  <p>This book covers aspects of human re-identification problems related to computer vision and machine learning. Working from a practical perspective, it introduces novel algorithms and designs for human re-identification that bridge the gap between research and reality. The primary focus is on building a robust, reliable, distributed and scalable smart surveillance system that can be deployed in real-world scenarios.  </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="tpami">
            <figure class="pub-card__media">
              <img src="poseprior.JPG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="http://dx.doi.org/10.1109/TPAMI.2014.2360373">Viewpoint Invariant Human Re-Identification in Camera Networks Using Pose Priors and Subject-Discriminative Features</a></h3>
            <div class="pub-card__meta"><span class="pub-card__meta-item">TPAMI</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We build a model for human
            appearance as a function of pose, using training data gathered from a calibrated camera. We then apply this &#8220;pose prior&#8221; in
            online re-identification to make matching and identification more robust to viewpoint. We further integrate person-specific features
            learned over the course of tracking to improve the algorithm&#8217;s performance. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="british-machine-vision-conference-(">
            <figure class="pub-card__media">
              <img src="LFDA.png" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/li-wacv15.pdf">Multi-Shot Human Re-Identification Using Adaptive Fisher Discriminant Analysis</a></h3>
            <p class="pub-card__authors"><a href="https://www.linkedin.com/in/yangli625">Yang Li</a>,<strong> Ziyan Wu</strong>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>,&nbsp; <span> <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></span>&nbsp;</p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">British Machine Vision Conference (</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We introduce an algorithm to
            hierarchically cluster image sequences and use the representative data samples to learn a
            feature subspace maximizing the Fisher criterion. The clustering and subspace learning
            processes are applied iteratively to obtain diversity-preserving discriminative features.
             </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="british-machine-vision-conference-(">
            <figure class="pub-card__media">
              <img src="vi.PNG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="http://www.bmva.org/bmvc/2014/papers/paper090/index.html">Virtual Insertion: Robust Bundle Adjustment over Long Video Sequences</a></h3>
            <p class="pub-card__authors"><strong>Ziyan Wu</strong>,<span><a href="https://scholar.google.com/citations?user=a-gkZ9kAAAAJ&amp;hl=en"> <span class="auto-style1">Han-Pang Chiu</span></a></span>, <a href="https://scholar.google.com/citations?hl=en&user=uCZgwJMAAAAJ"> <span class="auto-style1">Zhiwei Zhu</span></a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">British Machine Vision Conference (</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>We propose a novel &#8220;virtual insertion&#8221; scheme 
            					for Structure from Motion (SfM), which constructs virtual points and virtual frames to adapt the existence of visual landmark link
            outage, namely &#8220;visual breaks&#8221; due to no common features observed from neighboring
            camera views in challenging environments. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="rpi">
            <figure class="pub-card__media">
              <img src="rpi_thesis.png" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="http://digitool.rpi.edu:8881/R/PLD7F1DG68USTVSN2EXRI98EFUQE4FFYUSJYN7IJDLILKAVU3R-00305?func=dbin-jump-full&object_id=172696&local_base=GEN01&pds_handle=GUEST">Multi-Object Tracking and Association With a Camera Network</a></h3>
            <p class="pub-card__authors"><strong>Ziyan Wu</strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">RPI</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="prl">
            <figure class="pub-card__media">
              <img src="ctflow_prl.PNG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/wu-prl13.pdf">Improving Counterflow Detection in Dense Crowds with Scene Features</a></h3>
            <p class="pub-card__authors"><strong>Ziyan Wu</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">PRL</span></div>
          </header>
          <div class="pub-card__abstract">
            <p>This paper addresses the problem of detecting counterflow motion in
            videos of highly dense crowds. We focus on improving the detection performance by identifying scene features &#8212; that is, features on motionless
            background surfaces. We propose a three-way classifier to differentiate counterflow from normal flow, simultaneously identifying scene features based on
            statistics of low-level feature point tracks. </p>
          </div>
            </div>
          </article>
          <article class="pub-card" data-venue="acm/ieee-international-conference-on-distributed-smart-cameras-(">
            <figure class="pub-card__media">
              <img src="icdsc14.PNG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/radke-icdsc14.pdf">Real-World Re-Identification in an Airport Camera Network</a></h3>
            <p class="pub-card__authors"><a href="https://www.linkedin.com/in/yangli625">Yang Li</a>, <strong>Ziyan Wu</strong>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ACM/IEEE International Conference on Distributed Smart Cameras (</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="tpami">
            <figure class="pub-card__media">
              <img src="ptz.PNG" height="142" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/wu-pami12.pdf">Keeping a PTZ Camera Calibrated</a></h3>
            <div class="pub-card__meta"><span class="pub-card__meta-item">TPAMI</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="cvprw">
            <figure class="pub-card__media">
              <img src="cnwasa12.PNG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/wu-cnwasa12.pdf">Using Scene Features to Improve Wide-Area Video Surveillance</a></h3>
            <p class="pub-card__authors"><strong>Ziyan Wu</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">CVPRW</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="cvprw">
            <figure class="pub-card__media">
              <img src="airport.PNG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.ecse.rpi.edu/~rjradke/papers/wucnwasa11.pdf">Real-Time Airport Security Checkpoint Surveillance Using a Camera Network</a></h3>
            <p class="pub-card__authors"><strong>Ziyan Wu</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm">Richard J. Radke</a></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">CVPRW</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="icdar">
            <figure class="pub-card__media">
              <img src="icdar11.PNG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.researchgate.net/publication/220861297_Towards_Improved_Paper-Based_Election_Technology">Towards Improved Paper-based Election Technology</a></h3>
            <p class="pub-card__authors"><a href="http://coen.boisestate.edu/EBarneySmith/">Elisa Barney Smith</a>, <a href="http://www.cse.lehigh.edu/~lopresti/">Daniel Lopresti</a>, <a href="https://www.ecse.rpi.edu/~nagy/">George Nagy</a>, <strong>Ziyan Wu </strong></p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">ICDAR</span></div>
          </header>
            </div>
          </article>
          <article class="pub-card" data-venue="drr">
            <figure class="pub-card__media">
              <img src="drr2011.PNG" height="156" width="156">
            </figure>
            <div class="pub-card__content">
          <header class="pub-card__header">
            <h3 class="pub-card__title"><a href="https://www.researchgate.net/publication/221253858_Characterizing_Challenged_Minnesota_Ballots">Characterizing Challenged Minnesota Ballots</a></h3>
            <p class="pub-card__authors"><a href="https://www.ecse.rpi.edu/~nagy/">George Nagy</a>, <a href="http://www.cse.lehigh.edu/~lopresti/">Daniel Lopresti</a>, <a href="http://coen.boisestate.edu/EBarneySmith/">Elisa Barney Smith</a>, <strong>Ziyan Wu </strong> &nbsp;</p>
            <div class="pub-card__meta"><span class="pub-card__meta-item">DRR</span></div>
          </header>
            </div>
          </article>
              </div>
            </div>
          </div>
        </section>
        <section class="section" id="synergistic">
          <div class="section-inner">
            <div class="section-label">
              <h2 class="section-title">Synergistic Activities</h2>
            </div>
            <div class="section-body">
              <ul class="section-list">
                        <li>Associate Editor, <strong>IEEE Access</strong> (2017 - 2025)</li>
                        <li>Organizer, <a href="https://apahws.github.io/">Workshop on Advanced Perception for Autonomous Healthcare</a>, ICCV 2025</li>
                        <li>Organizer, <a href="https://fadetrcv.github.io/2025/">Workshop on Fair, Data Efficient and Trusted Computer Vision</a>, CVPR 2025</li>
                        <li>Organizer, <a href="https://fadetrcv.github.io/2024/">Workshop on Fair, Data Efficient and Trusted Computer Vision</a>, CVPR 2024</li>
                        <li>Organizer, <a href="https://aibsdworkshop.github.io/2024/index.html">Workshop on Artificial Intelligence with Biased or Scarce Data</a>, AAAI 2024</li>
                        <li>Organizer, <a href="https://fadetrcv.github.io/2023/">Workshop on Fair, Data Efficient and Trusted Computer Vision</a>, CVPR 2023</li>
                        <li>Organizer, <a href="https://wvbsd.github.io/2022/">Workshop on Vision with Biased and Scarce Data</a>, ECCV 2022</li>
                        <li>Organizer, <a href="https://fadetrcv.github.io/2022/">Workshop on Fair, Data Efficient and Trusted Computer Vision</a>, CVPR 2022 <strong class="pub-tag">(spotlight)</strong></li>
                        <li>Organizer, <a href="https://aibsdworkshop.github.io/2022/index.html">Workshop on Artificial Intelligence with Biased or Scarce Data</a>, AAAI 2022</li>
                        <li>Organizer, <a href="https://fadetrcv.github.io/2021/">Workshop on Fair, Data Efficient and Trusted Computer Vision</a>, CVPR 2021</li>
                        <li>Organizer, <a href="https://fadetrcv.github.io/2020/">Workshop on Fair, Data Efficient and Trusted Computer Vision</a>, CVPR 2020</li>
                        <li>Organizer, <a href="https://wvbsd.github.io/2019/index.html">Workshop on Vision with Biased and Scarce Data</a>, CVPR 2019</li>
                        <li>Organizer, <a href="https://wvbsd.github.io/2018/index.html">Workshop on Vision with Biased and Scarce Data</a>, CVPR 2018</li>
                        <li>Organizer, <a href="http://cvpr2017.thecvf.com/exhibit/spotlights">EXPO Spotlight</a>, CVPR 2017</li>
                        <li>Organizer, <a href="http://computervisioncentral.com/view/">Vision Industry and Entrepreneur Workshop</a>, CVPR 2016</li>
                      </ul>
            </div>
          </div>
        </section>
      </main>
    <footer>
      <div class="footer-links">
        <a href="mailto:ziyan@alum.rpi.edu">Email</a>
        <a href="https://scholar.google.com/citations?user=CkPUb-4AAAAJ">Google Scholar</a>
        <a href="http://www.linkedin.com/in/wuziyan/">LinkedIn</a>
      </div>
      <p class="footer-credit">Template inspired by <a href="https://jonbarron.info/">Jon Barron</a></p>
      <p class="footer-meta">&copy; <span id="copyright-year"></span> Ziyan Wu. All Rights Reserved.</p>
    </footer>
    
    <script>
      (function() {
        const motionPref = window.matchMedia('(prefers-reduced-motion: reduce)');
        const sections = document.querySelectorAll('.section');

        if (!motionPref.matches && 'IntersectionObserver' in window) {
          sections.forEach((section) => section.classList.add('will-animate'));
          const observer = new IntersectionObserver((entries) => {
            entries.forEach((entry) => {
              if (entry.isIntersecting) {
                entry.target.classList.add('reveal');
                entry.target.classList.remove('will-animate');
                observer.unobserve(entry.target);
              }
            });
          }, { threshold: 0.12 });

          sections.forEach((section) => observer.observe(section));
        } else {
          sections.forEach((section) => section.classList.add('reveal'));
        }

        const progressBar = document.querySelector('.scroll-progress');
        if (progressBar) {
          window.addEventListener('scroll', () => {
            const winScroll = document.documentElement.scrollTop || document.body.scrollTop;
            const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            const percent = height > 0 ? (winScroll / height) * 100 : 0;
            progressBar.style.width = percent + '%';
          });
        }

        const backToTop = document.createElement('button');
        backToTop.className = 'back-to-top';
        backToTop.innerHTML = '<i class="fa-solid fa-arrow-up"></i>';
        backToTop.setAttribute('aria-label', 'Back to top');
        document.body.appendChild(backToTop);

        const toggleBackToTop = () => {
          if (window.pageYOffset > 300) {
            backToTop.classList.add('visible');
          } else {
            backToTop.classList.remove('visible');
          }
        };

        toggleBackToTop();
        window.addEventListener('scroll', toggleBackToTop);

        backToTop.addEventListener('click', () => {
          window.scrollTo({ top: 0, behavior: motionPref.matches ? 'auto' : 'smooth' });
        });

        const navToggle = document.querySelector('.mobile-nav-toggle');
        const navMenu = document.querySelector('.site-nav');
        if (navToggle && navMenu) {
          navToggle.addEventListener('click', () => {
            const isOpen = navMenu.classList.toggle('mobile-open');
            const icon = navToggle.querySelector('i');
            if (icon) {
              icon.className = isOpen ? 'fa-solid fa-xmark' : 'fa-solid fa-bars';
            }
          });

          navMenu.querySelectorAll('a').forEach((link) => {
            link.addEventListener('click', () => {
              if (navMenu.classList.contains('mobile-open')) {
                navMenu.classList.remove('mobile-open');
                const icon = navToggle.querySelector('i');
                if (icon) {
                  icon.className = 'fa-solid fa-bars';
                }
              }
            });
          });
        }

        const chips = document.querySelectorAll('.pubs-chip');
        const cards = document.querySelectorAll('.pub-card');
        if (chips.length && cards.length) {
          const applyFilter = (filter) => {
            cards.forEach((card) => {
              const year = parseInt(card.dataset.year || '0', 10);
              const isHighlight = card.dataset.flag === 'highlight';
              let visible = true;

              if (filter === 'highlight') {
                visible = isHighlight;
              } else if (filter === 'earlier') {
                visible = year > 0 ? year < 2024 : true;
              } else if (filter && filter !== 'all') {
                visible = card.dataset.year === filter;
              }

              card.classList.toggle('is-hidden', !visible);
            });
          };

          chips.forEach((chip) => {
            chip.setAttribute('aria-pressed', chip.classList.contains('is-active') ? 'true' : 'false');
            chip.addEventListener('click', () => {
              chips.forEach((btn) => {
                btn.classList.remove('is-active');
                btn.setAttribute('aria-pressed', 'false');
              });
              chip.classList.add('is-active');
              chip.setAttribute('aria-pressed', 'true');
              applyFilter(chip.dataset.filter || 'all');
            });
          });

          applyFilter('all');
        }

        const copyrightYear = document.getElementById('copyright-year');
        if (copyrightYear) {
          copyrightYear.textContent = new Date().getFullYear();
        }
        }
      })();
    </script>

  </body>
</html>
